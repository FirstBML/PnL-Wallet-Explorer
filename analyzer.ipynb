{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d5ef43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Moralis API connection successful!\n",
      "\n",
      "🔍 Testing 5 known active wallets...\n",
      "\n",
      "--- Wallet 1/3 ---\n",
      "\n",
      "Analyzing wallet: 0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045\n",
      "  Checking eth...\n",
      "    eth: 50 native, 50 ERC20\n",
      "  Checking bsc...\n",
      "    bsc: 50 native, 50 ERC20\n",
      "  Checking polygon...\n",
      "    polygon: 50 native, 50 ERC20\n",
      "  Checking arbitrum...\n",
      "    arbitrum: 50 native, 50 ERC20\n",
      "  Checking optimism...\n",
      "    optimism: 50 native, 50 ERC20\n",
      "  Checking base...\n",
      "    base: 50 native, 50 ERC20\n",
      "  ✅ QUALIFIED: 600 total activities, 6 chains\n",
      "\n",
      "--- Wallet 2/3 ---\n",
      "\n",
      "Analyzing wallet: 0x28C6c06298d514Db089934071355E5743bf21d60\n",
      "  Checking eth...\n",
      "    eth: 50 native, 50 ERC20\n",
      "  Checking bsc...\n",
      "    bsc: 50 native, 50 ERC20\n",
      "  Checking polygon...\n",
      "    polygon: 17 native, 50 ERC20\n",
      "  Checking arbitrum...\n",
      "    arbitrum: 13 native, 50 ERC20\n",
      "  Checking optimism...\n",
      "    optimism: 4 native, 50 ERC20\n",
      "  Checking base...\n",
      "    base: 50 native, 50 ERC20\n",
      "  ✅ QUALIFIED: 484 total activities, 6 chains\n",
      "\n",
      "--- Wallet 3/3 ---\n",
      "\n",
      "Analyzing wallet: 0xF977814e90dA44bFA03b6295A0616a897441aceC\n",
      "  Checking eth...\n",
      "    eth: 50 native, 50 ERC20\n",
      "  Checking bsc...\n",
      "    bsc: 50 native, 50 ERC20\n",
      "  Checking polygon...\n",
      "    polygon: 50 native, 50 ERC20\n",
      "  Checking arbitrum...\n",
      "    arbitrum: 50 native, 50 ERC20\n",
      "  Checking optimism...\n",
      "    optimism: 50 native, 50 ERC20\n",
      "  Checking base...\n",
      "    base: 50 native, 50 ERC20\n",
      "  ✅ QUALIFIED: 600 total activities, 6 chains\n",
      "\n",
      "✅ 3 wallets qualified!\n",
      "\n",
      "📊 Getting detailed transaction data...\n",
      "\n",
      "  Processing 0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045...\n",
      "    Getting eth data...\n",
      "    Getting bsc data...\n",
      "    Getting polygon data...\n",
      "    Getting arbitrum data...\n",
      "    Getting optimism data...\n",
      "    Getting base data...\n",
      "\n",
      "  Processing 0x28C6c06298d514Db089934071355E5743bf21d60...\n",
      "    Getting eth data...\n",
      "    Getting bsc data...\n",
      "    Getting polygon data...\n",
      "    Getting arbitrum data...\n",
      "    Getting optimism data...\n",
      "    Getting base data...\n",
      "\n",
      "  Processing 0xF977814e90dA44bFA03b6295A0616a897441aceC...\n",
      "    Getting eth data...\n",
      "    Getting bsc data...\n",
      "    Getting polygon data...\n",
      "    Getting arbitrum data...\n",
      "    Getting optimism data...\n",
      "    Getting base data...\n",
      "\n",
      "🎉 SUCCESS! Collected 1684 transactions\n",
      "💾 Data saved to 'moralis_wallet_data.csv'\n",
      "\n",
      "Sample data:\n",
      "                                             tx_hash  \\\n",
      "0  0x6645bf8a80fb3b08aaf3fc30418ae51f26014a8e12bd...   \n",
      "1  0x6c1ca6ef4485dacb1342109ee84b9bfce0e16e519719...   \n",
      "2  0x0b2e99577678600fd37eb5f094d17c8dbc55de67f807...   \n",
      "3  0xdd82db1016b9834f3c8094a9eb4c0e879f0ef37bcc2e...   \n",
      "4  0x6b9a4e2e3213f7124241b574fd44382418d31d2b8a3c...   \n",
      "\n",
      "                                       wallet blockchain           action  \\\n",
      "0  0x28C6c06298d514Db089934071355E5743bf21d60    polygon  native_transfer   \n",
      "1  0x28C6c06298d514Db089934071355E5743bf21d60    polygon  native_transfer   \n",
      "2  0x28C6c06298d514Db089934071355E5743bf21d60    polygon  native_transfer   \n",
      "3  0x28C6c06298d514Db089934071355E5743bf21d60    polygon  native_transfer   \n",
      "4  0x28C6c06298d514Db089934071355E5743bf21d60    polygon  native_transfer   \n",
      "\n",
      "  transaction_type  \n",
      "0          deposit  \n",
      "1          deposit  \n",
      "2          deposit  \n",
      "3          deposit  \n",
      "4          deposit  \n",
      "\n",
      "📈 Final Results Summary:\n",
      "Total transactions: 1684\n",
      "Unique wallets: 3\n",
      "Blockchains: 6\n",
      "Date range: 2021-07-29 00:19:06+00:00 to 2025-09-03 06:50:11+00:00\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "\n",
    "API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJub25jZSI6IjdlMjBjNDk0LWU0MjAtNGFmOC05MzM2LTkxNTBjNDU3MmJjZCIsIm9yZ0lkIjoiNDY4ODE1IiwidXNlcklkIjoiNDgyMjkyIiwidHlwZUlkIjoiMDk3MTE4YTItNWVkOC00Yjc2LTg5YWItMjM5NDgzNDVjYzNiIiwidHlwZSI6IlBST0pFQ1QiLCJpYXQiOjE3NTY4NDM1NjIsImV4cCI6NDkxMjYwMzU2Mn0.yLn2ojeo6b4qJA9IYnSJlel5gZVlJChuZbhqkUBSLeo\"\n",
    "\n",
    "class FixedMoralisAnalyzer:\n",
    "    def __init__(self, api_key: str, cache_file: str = \"moralis_cache.json\"):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://deep-index.moralis.io/api/v2\"\n",
    "        self.headers = {\n",
    "            \"Accept\": \"application/json\",\n",
    "            \"X-API-Key\": api_key\n",
    "        }\n",
    "        self.chains = {\n",
    "            'eth': '0x1',\n",
    "            'bsc': '0x38', \n",
    "            'polygon': '0x89',\n",
    "            'arbitrum': '0xa4b1',\n",
    "            'optimism': '0xa',\n",
    "            'base': '0x2105'\n",
    "        }\n",
    "\n",
    "        # Initialize cache\n",
    "        self.cache_file = cache_file\n",
    "        if os.path.exists(cache_file):\n",
    "            with open(cache_file, \"r\") as f:\n",
    "                try:\n",
    "                    self.cache = json.load(f)\n",
    "                except:\n",
    "                    self.cache = {}\n",
    "        else:\n",
    "            self.cache = {}\n",
    "\n",
    "    def _make_request(self, endpoint: str, params: dict) -> dict:\n",
    "        \"\"\"Helper with caching logic\"\"\"\n",
    "        key = hashlib.sha256((endpoint + json.dumps(params, sort_keys=True)).encode()).hexdigest()\n",
    "\n",
    "        if key in self.cache:\n",
    "            # ✅ Cached response\n",
    "            return self.cache[key]\n",
    "\n",
    "        # ❌ Not cached → API call\n",
    "        url = f\"{self.base_url}{endpoint}\"\n",
    "        response = requests.get(url, headers=self.headers, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            # Save to cache\n",
    "            self.cache[key] = data\n",
    "            with open(self.cache_file, \"w\") as f:\n",
    "                json.dump(self.cache, f)\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"API Error {response.status_code}: {response.text[:200]}\")\n",
    "            return {}\n",
    "\n",
    "\n",
    "        \n",
    "    def test_connection(self) -> bool:\n",
    "        \"\"\"Test if API key works with native balance endpoint\"\"\"\n",
    "        try:\n",
    "            url = f\"{self.base_url}/0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045/balance\"\n",
    "            params = {\"chain\": \"0x1\"}  # Use hex chain ID\n",
    "            \n",
    "            response = requests.get(url, headers=self.headers, params=params)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                print(\"✅ Moralis API connection successful!\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"❌ API Error: {response.status_code}\")\n",
    "                print(f\"Response: {response.text}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Connection error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_native_transactions(self, wallet: str, chain: str = '0x1', limit: int = 100) -> List[Dict]:\n",
    "        endpoint = f\"/{wallet}\"\n",
    "        params = {\"chain\": chain, \"limit\": limit}\n",
    "        data = self._make_request(endpoint, params)\n",
    "        return data.get('result', [])\n",
    "\n",
    "    def get_erc20_transfers(self, wallet: str, chain: str = '0x1', limit: int = 100) -> List[Dict]:\n",
    "        endpoint = f\"/{wallet}/erc20/transfers\"\n",
    "        params = {\"chain\": chain, \"limit\": limit}\n",
    "        data = self._make_request(endpoint, params)\n",
    "        return data.get('result', [])\n",
    "\n",
    "    def analyze_single_wallet(self, wallet: str) -> Optional[Dict]:\n",
    "        \"\"\"Analyze a single wallet across all chains\"\"\"\n",
    "        print(f\"\\nAnalyzing wallet: {wallet}\")\n",
    "        \n",
    "        wallet_stats = {\n",
    "            'wallet': wallet,\n",
    "            'total_native_txs': 0,\n",
    "            'total_erc20_transfers': 0,\n",
    "            'active_chains': 0,\n",
    "            'chain_details': {}\n",
    "        }\n",
    "        \n",
    "        for chain_name, chain_id in self.chains.items():\n",
    "            print(f\"  Checking {chain_name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Get native transactions\n",
    "                native_txs = self.get_native_transactions(wallet, chain_id, limit=50)\n",
    "                \n",
    "                # Get ERC20 transfers  \n",
    "                erc20_transfers = self.get_erc20_transfers(wallet, chain_id, limit=50)\n",
    "                \n",
    "                chain_native_count = len(native_txs)\n",
    "                chain_erc20_count = len(erc20_transfers)\n",
    "                \n",
    "                if chain_native_count > 0 or chain_erc20_count > 0:\n",
    "                    wallet_stats['active_chains'] += 1\n",
    "                    wallet_stats['total_native_txs'] += chain_native_count\n",
    "                    wallet_stats['total_erc20_transfers'] += chain_erc20_count\n",
    "                    \n",
    "                    wallet_stats['chain_details'][chain_name] = {\n",
    "                        'native_txs': chain_native_count,\n",
    "                        'erc20_transfers': chain_erc20_count\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"    {chain_name}: {chain_native_count} native, {chain_erc20_count} ERC20\")\n",
    "                \n",
    "                # Rate limiting\n",
    "                time.sleep(0.3)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error with {chain_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Check qualification criteria (simplified for testing)\n",
    "        total_activity = wallet_stats['total_native_txs'] + wallet_stats['total_erc20_transfers']\n",
    "        \n",
    "        if total_activity >= 10 and wallet_stats['active_chains'] >= 2:\n",
    "            print(f\"  ✅ QUALIFIED: {total_activity} total activities, {wallet_stats['active_chains']} chains\")\n",
    "            return wallet_stats\n",
    "        else:\n",
    "            print(f\"  ❌ Not qualified: {total_activity} activities, {wallet_stats['active_chains']} chains\")\n",
    "            return None\n",
    "    \n",
    "    def get_detailed_data_for_wallet(self, wallet: str, max_per_chain: int = 100) -> List[Dict]:\n",
    "        \"\"\"Get detailed transaction data for a qualified wallet\"\"\"\n",
    "        all_transactions = []\n",
    "        \n",
    "        for chain_name, chain_id in self.chains.items():\n",
    "            try:\n",
    "                print(f\"    Getting {chain_name} data...\")\n",
    "                \n",
    "                # Get native transactions\n",
    "                native_txs = self.get_native_transactions(wallet, chain_id, limit=max_per_chain)\n",
    "                \n",
    "                for tx in native_txs:\n",
    "                    processed = {\n",
    "                        'tx_hash': tx.get('hash'),\n",
    "                        'block_time': tx.get('block_timestamp'),\n",
    "                        'wallet': wallet,\n",
    "                        'blockchain': chain_name,\n",
    "                        'from_address': tx.get('from_address'),\n",
    "                        'to_address': tx.get('to_address'),\n",
    "                        'value_wei': tx.get('value', '0'),\n",
    "                        'value_native': float(tx.get('value', '0')) / 1e18 if tx.get('value') else 0,\n",
    "                        'gas_used': tx.get('gas_used'),\n",
    "                        'gas_price': tx.get('gas_price'),\n",
    "                        'action': 'native_transfer',\n",
    "                        'transaction_type': 'deposit' if tx.get('to_address', '').lower() == wallet.lower() else 'withdrawal'\n",
    "                    }\n",
    "                    all_transactions.append(processed)\n",
    "                \n",
    "                # Get ERC20 transfers\n",
    "                erc20_transfers = self.get_erc20_transfers(wallet, chain_id, limit=max_per_chain)\n",
    "                \n",
    "                for transfer in erc20_transfers:\n",
    "                    processed = {\n",
    "                        'tx_hash': transfer.get('transaction_hash'),\n",
    "                        'block_time': transfer.get('block_timestamp'),\n",
    "                        'wallet': wallet,\n",
    "                        'blockchain': chain_name,\n",
    "                        'from_address': transfer.get('from_address'),\n",
    "                        'to_address': transfer.get('to_address'),\n",
    "                        'token_address': transfer.get('address'),\n",
    "                        'token_symbol': transfer.get('token_symbol'),\n",
    "                        'token_name': transfer.get('token_name'),\n",
    "                        'value_raw': transfer.get('value', '0'),\n",
    "                        'decimals': transfer.get('token_decimals', '18'),\n",
    "                        'action': 'erc20_transfer',\n",
    "                        'transaction_type': 'deposit' if transfer.get('to_address', '').lower() == wallet.lower() else 'withdrawal'\n",
    "                    }\n",
    "                    \n",
    "                    # Calculate human-readable amount\n",
    "                    try:\n",
    "                        decimals = int(transfer.get('token_decimals', '18'))\n",
    "                        raw_value = float(transfer.get('value', '0'))\n",
    "                        processed['amount'] = raw_value / (10 ** decimals)\n",
    "                    except:\n",
    "                        processed['amount'] = 0\n",
    "                    \n",
    "                    all_transactions.append(processed)\n",
    "                \n",
    "                time.sleep(0.5)  # Rate limiting\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error getting {chain_name} details: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return all_transactions\n",
    "\n",
    "def run_fixed_moralis_analysis(api_key: str, max_wallets: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"Run the complete analysis with fixed endpoints\"\"\"\n",
    "    \n",
    "    analyzer = FixedMoralisAnalyzer(api_key)\n",
    "    \n",
    "    # Test connection\n",
    "    if not analyzer.test_connection():\n",
    "        print(\"❌ Connection failed. Check your API key.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Test wallets (known active addresses)\n",
    "    test_wallets = [\n",
    "        \"0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045\",  # Vitalik\n",
    "        \"0x28C6c06298d514Db089934071355E5743bf21d60\",  # Binance\n",
    "        \"0xF977814e90dA44bFA03b6295A0616a897441aceC\",  # Alameda\n",
    "        \"0x3fC91A3afd70395Cd496C647d5a6CC9D4B2b7FAD\",  # Uniswap\n",
    "        \"0x1111111254fb6c44bAC0beD2854e76F90643097d\",  # 1inch\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n🔍 Testing {len(test_wallets)} known active wallets...\")\n",
    "    \n",
    "    qualified_wallets = []\n",
    "    \n",
    "    for i, wallet in enumerate(test_wallets[:max_wallets], 1):\n",
    "        print(f\"\\n--- Wallet {i}/{max_wallets} ---\")\n",
    "        result = analyzer.analyze_single_wallet(wallet)\n",
    "        \n",
    "        if result:\n",
    "            qualified_wallets.append(result)\n",
    "        \n",
    "        # Rate limiting between wallets\n",
    "        if i < len(test_wallets):\n",
    "            time.sleep(2)\n",
    "    \n",
    "    if not qualified_wallets:\n",
    "        print(\"\\n❌ No wallets qualified\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\n✅ {len(qualified_wallets)} wallets qualified!\")\n",
    "    print(\"\\n📊 Getting detailed transaction data...\")\n",
    "    \n",
    "    # Get detailed data\n",
    "    all_detailed_data = []\n",
    "    \n",
    "    for wallet_info in qualified_wallets:\n",
    "        wallet = wallet_info['wallet']\n",
    "        print(f\"\\n  Processing {wallet}...\")\n",
    "        \n",
    "        wallet_transactions = analyzer.get_detailed_data_for_wallet(wallet, max_per_chain=50)\n",
    "        all_detailed_data.extend(wallet_transactions)\n",
    "        \n",
    "        time.sleep(1)  # Rate limiting between wallets\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    if all_detailed_data:\n",
    "        df = pd.DataFrame(all_detailed_data)\n",
    "        df['block_time'] = pd.to_datetime(df['block_time'])\n",
    "        df = df.sort_values('block_time').reset_index(drop=True)\n",
    "        \n",
    "        print(f\"\\n🎉 SUCCESS! Collected {len(df)} transactions\")\n",
    "        \n",
    "        # Save results\n",
    "        df.to_csv('moralis_wallet_data.csv', index=False)\n",
    "        print(\"💾 Data saved to 'moralis_wallet_data.csv'\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(f\"\\nSample data:\")\n",
    "        print(df[['tx_hash', 'wallet', 'blockchain', 'action', 'transaction_type']].head())\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(\"❌ No transaction data collected\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# USAGE:\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual Moralis API key\n",
    "\n",
    "\n",
    "    if API_KEY == \"MORALIS_API_KEY\":\n",
    "        print(\"⚠️  Please set your actual Moralis API key\")\n",
    "    else:\n",
    "        result = run_fixed_moralis_analysis(API_KEY, max_wallets=3)\n",
    "        \n",
    "        if not result.empty:\n",
    "            print(f\"\\n📈 Final Results Summary:\")\n",
    "            print(f\"Total transactions: {len(result)}\")\n",
    "            print(f\"Unique wallets: {result['wallet'].nunique()}\")\n",
    "            print(f\"Blockchains: {result['blockchain'].nunique()}\")\n",
    "            print(f\"Date range: {result['block_time'].min()} to {result['block_time'].max()}\")\n",
    "        else:\n",
    "            print(\"No data collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae7d7bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# -------------------------------\n",
    "# Load environment variables\n",
    "# -------------------------------\n",
    "load_dotenv()\n",
    "MORALIS_API_KEY = os.getenv(\"MORALIS_API_KEY\")\n",
    "COINGECKO_API_KEY = os.getenv(\"GECKO_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0325d735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching ERC20 transfers on eth...\n",
      "Fetching ERC20 transfers on bsc...\n",
      "Fetching ERC20 transfers on polygon...\n",
      "Fetching ERC20 transfers on arbitrum...\n",
      "Fetching ERC20 transfers on optimism...\n",
      "Fetching ERC20 transfers on base...\n",
      "                                        wallet blockchain  \\\n",
      "0   0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045    polygon   \n",
      "1   0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045    polygon   \n",
      "2   0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045    polygon   \n",
      "3   0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045    polygon   \n",
      "4   0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045    polygon   \n",
      "5   0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045    polygon   \n",
      "6   0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045    polygon   \n",
      "7   0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045    polygon   \n",
      "8   0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045    polygon   \n",
      "9   0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045    polygon   \n",
      "10  0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045    polygon   \n",
      "11  0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045    polygon   \n",
      "12  0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045    polygon   \n",
      "13  0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045    polygon   \n",
      "14  0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045   optimism   \n",
      "15  0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045   optimism   \n",
      "16  0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045   optimism   \n",
      "17  0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045   optimism   \n",
      "18  0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045   optimism   \n",
      "19  0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045   optimism   \n",
      "\n",
      "                                              tx_hash  \\\n",
      "0   0x1b1792a6ea6c0ca4f4440181b6a4a1cbbff085043066...   \n",
      "1   0xfbc546350589442458fa38ec85aa63d8d26348e9ff04...   \n",
      "2   0x6d893b80bb008160897c59ae2dc0e5c224e0e91d2fc4...   \n",
      "3   0x29ec470327f66f070c0f971b605f84118c3801ba3ae4...   \n",
      "4   0x880ddadd55025c622a3ee29d94c9c873f46546b8f65f...   \n",
      "5   0x97352ab4afaa4a72a07d5cce4cf25b2946c2d46b4ca7...   \n",
      "6   0xec2443415286e11885622393948ddbb2921e3f63ae4f...   \n",
      "7   0x85dea7155d6bb7f0daf1fc0bfbf0b4c1206ad0942501...   \n",
      "8   0xbad5e36d7d7d609b3925ba9741bb863c280c187f9106...   \n",
      "9   0x98d14773fa1952635e95094a76175ce9b7087a6b6eb3...   \n",
      "10  0x183ed01a041e0fcd9a910dada05f641492dc7610ece0...   \n",
      "11  0xef3de95d326cb14e4a2b9be727b44b46eb1c57d7f962...   \n",
      "12  0x5b3e5f6137c5d1e286a43e22c6cb15389ca5d0ab78a4...   \n",
      "13  0xaa4a674d9c73772641559b70d3a6f026c9db44a822c1...   \n",
      "14  0xdb23645a38e78ac50edb5aa210c76a3eb08f11bf914c...   \n",
      "15  0xf6951d2ebb9b958140b30d2bb9c003bcefa2b80459e7...   \n",
      "16  0x0141ef931c12ba382731c137555e6ce42ad6f047fc8b...   \n",
      "17  0x32b520b4568a48db356d02ba39e6780977b83a7dc2ef...   \n",
      "18  0x06db663e0d6aefe087c2d86372fc590b49894a697b88...   \n",
      "19  0x4e7a1a73d43f4bd83421631978ce1906d4d3bab37708...   \n",
      "\n",
      "                  block_time  \\\n",
      "0  2025-06-09 02:23:00+00:00   \n",
      "1  2025-06-09 06:14:45+00:00   \n",
      "2  2025-06-09 06:14:55+00:00   \n",
      "3  2025-06-09 06:15:03+00:00   \n",
      "4  2025-06-09 06:15:11+00:00   \n",
      "5  2025-06-12 19:42:01+00:00   \n",
      "6  2025-06-15 14:24:09+00:00   \n",
      "7  2025-06-16 18:10:02+00:00   \n",
      "8  2025-06-22 06:38:30+00:00   \n",
      "9  2025-06-23 12:11:33+00:00   \n",
      "10 2025-06-24 02:41:47+00:00   \n",
      "11 2025-07-06 19:24:09+00:00   \n",
      "12 2025-07-08 15:54:33+00:00   \n",
      "13 2025-07-08 18:52:47+00:00   \n",
      "14 2025-07-15 18:02:41+00:00   \n",
      "15 2025-07-15 18:02:43+00:00   \n",
      "16 2025-07-15 18:02:45+00:00   \n",
      "17 2025-07-15 18:02:51+00:00   \n",
      "18 2025-07-15 18:02:53+00:00   \n",
      "19 2025-07-15 18:06:53+00:00   \n",
      "\n",
      "                                         token_symbol  \\\n",
      "0                                 bigbossgarments.com   \n",
      "1   Please visit bigbossgarments.com | &#x2705; VE...   \n",
      "2   Please visit bigbossgarments.com | &#x2705; VE...   \n",
      "3   Please visit bigbossgarments.com | &#x2705; VE...   \n",
      "4   Please visit bigbossgarments.com | &#x2705; VE...   \n",
      "5                                                USDC   \n",
      "6                                                SWOL   \n",
      "7                                                 BSC   \n",
      "8                                               UERII   \n",
      "9                                                 DAI   \n",
      "10                                                 P3   \n",
      "11                                               SWOL   \n",
      "12  $BASE Visit WWW.BASETERMINAL.SHOP to claim reward   \n",
      "13  $BASE Visit WWW.BASETERMINAL.SHOP to claim reward   \n",
      "14                                                 TT   \n",
      "15                                                 TT   \n",
      "16                                                 TT   \n",
      "17                                                 TT   \n",
      "18                                                 TT   \n",
      "19                                                 TT   \n",
      "\n",
      "                                 token_address        amount  price_usd  \\\n",
      "0   0xa429d00f5b3d304d9eb2a6c0ccc7d7232364d59b   10000.00000        0.0   \n",
      "1   0x5c2942a493dc53cd89ba656c3da1180ffcc59dab  100000.00000        0.0   \n",
      "2   0x5c2942a493dc53cd89ba656c3da1180ffcc59dab  100000.00000        0.0   \n",
      "3   0x5c2942a493dc53cd89ba656c3da1180ffcc59dab  100000.00000        0.0   \n",
      "4   0x5c2942a493dc53cd89ba656c3da1180ffcc59dab  100000.00000        0.0   \n",
      "5   0x3c499c542cef5e3811e1192ce70d8cc03d5c3359       0.18518        0.0   \n",
      "6   0xa1ca6299ba48366af1845a9a8ae59b87ff0d5c01     150.00000        0.0   \n",
      "7   0x8701b4c17fce6128c7b95904f7ca79abfe813ce6      10.00000        0.0   \n",
      "8   0xd566c529b33ecf15170f600d4b1ab12468c8efc6    5000.00000        0.0   \n",
      "9   0x8f3cf7ad23cd3cadbd9735aff958023239c6a063       0.01000        0.0   \n",
      "10  0xbef18f56a7c848d530af1e97b033980cd6cb4a76       3.73571        0.0   \n",
      "11  0xa1ca6299ba48366af1845a9a8ae59b87ff0d5c01     200.00000        0.0   \n",
      "12  0x489e93e7e2d90a8e3552d6862d757b1088803094   18506.77000        0.0   \n",
      "13  0x803bdb1d11c5ca39054a9591d849bcbec055378c  120000.00000        0.0   \n",
      "14  0x91cd2a4a8c7c5ed4661148ce795cd12b1687a810      10.00000        0.0   \n",
      "15  0x91cd2a4a8c7c5ed4661148ce795cd12b1687a810      10.00000        0.0   \n",
      "16  0x91cd2a4a8c7c5ed4661148ce795cd12b1687a810      10.00000        0.0   \n",
      "17  0x91cd2a4a8c7c5ed4661148ce795cd12b1687a810      10.00000        0.0   \n",
      "18  0x91cd2a4a8c7c5ed4661148ce795cd12b1687a810      10.00000        0.0   \n",
      "19  0x91cd2a4a8c7c5ed4661148ce795cd12b1687a810      10.00000        0.0   \n",
      "\n",
      "    usd_value transaction_type  \n",
      "0         0.0          deposit  \n",
      "1         0.0          deposit  \n",
      "2         0.0          deposit  \n",
      "3         0.0          deposit  \n",
      "4         0.0          deposit  \n",
      "5         0.0          deposit  \n",
      "6         0.0          deposit  \n",
      "7         0.0          deposit  \n",
      "8         0.0          deposit  \n",
      "9         0.0          deposit  \n",
      "10        0.0          deposit  \n",
      "11        0.0          deposit  \n",
      "12        0.0          deposit  \n",
      "13        0.0          deposit  \n",
      "14        0.0          deposit  \n",
      "15        0.0          deposit  \n",
      "16        0.0          deposit  \n",
      "17        0.0          deposit  \n",
      "18        0.0          deposit  \n",
      "19        0.0          deposit  \n",
      "\n",
      "Summary USD values:\n",
      "transaction_type\n",
      "deposit       0.116087\n",
      "withdrawal    0.000000\n",
      "Name: usd_value, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# -------------------------------\n",
    "# Cache for prices\n",
    "# -------------------------------\n",
    "class PriceCache:\n",
    "    def __init__(self, filename=\"price_cache.json\"):\n",
    "        self.filename = filename\n",
    "        self.cache = {}\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, \"r\") as f:\n",
    "                try:\n",
    "                    self.cache = json.load(f)\n",
    "                except:\n",
    "                    self.cache = {}\n",
    "\n",
    "    def get(self, key: str):\n",
    "        return self.cache.get(key)\n",
    "\n",
    "    def set(self, key: str, value):\n",
    "        self.cache[key] = value\n",
    "        with open(self.filename, \"w\") as f:\n",
    "            json.dump(self.cache, f)\n",
    "\n",
    "# -------------------------------\n",
    "# Cache for contract-to-CGID mapping\n",
    "# -------------------------------\n",
    "class AddressCache:\n",
    "    def __init__(self, filename=\"address_to_cgid.json\"):\n",
    "        self.filename = filename\n",
    "        self.cache = {}\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, \"r\") as f:\n",
    "                try:\n",
    "                    self.cache = json.load(f)\n",
    "                except:\n",
    "                    self.cache = {}\n",
    "\n",
    "    def get(self, key: str):\n",
    "        return self.cache.get(key)\n",
    "\n",
    "    def set(self, key: str, value):\n",
    "        self.cache[key] = value\n",
    "        with open(self.filename, \"w\") as f:\n",
    "            json.dump(self.cache, f)\n",
    "\n",
    "# -------------------------------\n",
    "# Extended Analyzer\n",
    "# -------------------------------\n",
    "# -------------------------------\n",
    "# Extended Analyzer\n",
    "# -------------------------------\n",
    "class ExtendedMoralisAnalyzer:\n",
    "    def __init__(self, api_key: str, use_cache: bool = True, force_refresh: bool = False):\n",
    "        \"\"\"\n",
    "        :param api_key: Moralis API key\n",
    "        :param use_cache: if True, prefer cache\n",
    "        :param force_refresh: if True, ignore cache and refresh from API\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://deep-index.moralis.io/api/v2\"\n",
    "        self.headers = {\"Accept\": \"application/json\", \"X-API-Key\": api_key}\n",
    "        self.use_cache = use_cache\n",
    "        self.force_refresh = force_refresh\n",
    "\n",
    "        self.chains = {\n",
    "            'eth': '0x1',\n",
    "            'bsc': '0x38',\n",
    "            'polygon': '0x89',\n",
    "            'arbitrum': '0xa4b1',\n",
    "            'optimism': '0xa',\n",
    "            'base': '0x2105'\n",
    "        }\n",
    "\n",
    "        self.price_cache = PriceCache()\n",
    "        self.address_cache = AddressCache()\n",
    "        self.moralis_cache = ExtendedMoralisAnalyzer()\n",
    "\n",
    "    # -------------------------------\n",
    "    # Fetch ERC20 Transfers (cached)\n",
    "    # -------------------------------\n",
    "    def get_erc20_transfers(self, wallet: str, chain: str, limit: int = 50) -> List[Dict]:\n",
    "        cache_key = f\"{wallet}_{chain}\"\n",
    "        cached = self.moralis_cache.get(wallet, chain)\n",
    "\n",
    "        if self.use_cache and cached and not self.force_refresh:\n",
    "            return cached\n",
    "\n",
    "        try:\n",
    "            url = f\"{self.base_url}/{wallet}/erc20/transfers\"\n",
    "            params = {\"chain\": chain, \"limit\": limit}\n",
    "            response = requests.get(url, headers=self.headers, params=params)\n",
    "            if response.status_code == 200:\n",
    "                result = response.json().get('result', [])\n",
    "                if self.use_cache:\n",
    "                    self.moralis_cache.set(wallet, chain, result)\n",
    "                return result\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"ERC20 transfer error: {e}\")\n",
    "            return []\n",
    "\n",
    "    # -------------------------------\n",
    "    # Price Fetcher (cached)\n",
    "    # -------------------------------\n",
    "    def get_price_usd(self, symbol: str, timestamp: str, token_address: str = None, blockchain: str = \"ethereum\") -> Optional[float]:\n",
    "        if not symbol and not token_address:\n",
    "            return None\n",
    "\n",
    "        symbol = (symbol or \"\").lower()\n",
    "        date_str = timestamp.split(\"T\")[0]\n",
    "        cache_key = f\"{symbol}_{token_address}_{date_str}\"\n",
    "        cached = self.price_cache.get(cache_key)\n",
    "\n",
    "        if self.use_cache and cached and not self.force_refresh:\n",
    "            return cached\n",
    "\n",
    "        try:\n",
    "            mapping = {\n",
    "                \"eth\": \"ethereum\",\n",
    "                \"weth\": \"weth\",\n",
    "                \"usdc\": \"usd-coin\",\n",
    "                \"usdt\": \"tether\",\n",
    "                \"bnb\": \"binancecoin\",\n",
    "                \"matic\": \"polygon\"\n",
    "            }\n",
    "            cg_id = mapping.get(symbol)\n",
    "\n",
    "            if not cg_id and token_address:\n",
    "                cached_cgid = self.address_cache.get(token_address.lower())\n",
    "                if cached_cgid:\n",
    "                    cg_id = cached_cgid\n",
    "\n",
    "            if not cg_id and token_address:\n",
    "                try:\n",
    "                    url = f\"https://api.coingecko.com/api/v3/coins/{blockchain}/contract/{token_address}\"\n",
    "                    r = requests.get(url)\n",
    "                    if r.status_code == 200:\n",
    "                        data = r.json()\n",
    "                        cg_id = data.get(\"id\")\n",
    "                        if cg_id and self.use_cache:\n",
    "                            self.address_cache.set(token_address.lower(), cg_id)\n",
    "                except Exception as e:\n",
    "                    print(f\"Contract lookup failed for {token_address}: {e}\")\n",
    "\n",
    "            if not cg_id:\n",
    "                return None\n",
    "\n",
    "            url = f\"https://api.coingecko.com/api/v3/coins/{cg_id}/history\"\n",
    "            params = {\"date\": datetime.strptime(date_str, \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")}\n",
    "            r = requests.get(url, params=params)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                price = data.get(\"market_data\", {}).get(\"current_price\", {}).get(\"usd\")\n",
    "                if price and self.use_cache:\n",
    "                    self.price_cache.set(cache_key, price)\n",
    "                return price\n",
    "        except Exception as e:\n",
    "            print(f\"Price fetch error for {symbol} / {token_address}: {e}\")\n",
    "            return None\n",
    "    # -------------------------------\n",
    "    # GAS COSTS (from gas.fees table)\n",
    "    # -------------------------------\n",
    "    def get_gas_costs_for_wallet(self, wallet: str, max_per_chain: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetch gas fees for a given wallet across supported chains.\n",
    "        Returns DataFrame with tx_hash, gas_used, fee in USD, etc.\n",
    "        \"\"\"\n",
    "        all_gas = []\n",
    "        for chain_name, chain_id in self.chains.items():\n",
    "            try:\n",
    "                url = f\"{self.base_url}/{wallet}/transaction\"\n",
    "                params = {\"chain\": chain_id, \"limit\": max_per_chain}\n",
    "                response = requests.get(url, headers=self.headers, params=params)\n",
    "                if response.status_code != 200:\n",
    "                    continue\n",
    "\n",
    "                txs = response.json().get(\"result\", [])\n",
    "                for tx in txs:\n",
    "                    try:\n",
    "                        fee_usd = float(tx.get(\"gas_price\", 0)) * float(tx.get(\"receipt_gas_used\", 0)) / 1e18 * float(tx.get(\"usd_price\", 0))\n",
    "                        enriched = {\n",
    "                            \"wallet\": wallet,\n",
    "                            \"blockchain\": chain_name,\n",
    "                            \"tx_hash\": tx.get(\"hash\"),\n",
    "                            \"block_time\": tx.get(\"block_timestamp\"),\n",
    "                            \"gas_used\": tx.get(\"receipt_gas_used\"),\n",
    "                            \"gas_price\": tx.get(\"gas_price\"),\n",
    "                            \"gas_fee_usd\": fee_usd,\n",
    "                        }\n",
    "                        all_gas.append(enriched)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Gas enrich error: {e}\")\n",
    "                        continue\n",
    "            except Exception as e:\n",
    "                print(f\"Gas fetch error: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not all_gas:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(all_gas)\n",
    "        df['block_time'] = pd.to_datetime(df['block_time'])\n",
    "        return df.sort_values(\"block_time\").reset_index(drop=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Runner\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    load_dotenv()\n",
    "    API_KEY = os.getenv(\"MORALIS_API_KEY\")\n",
    "\n",
    "    if not API_KEY:\n",
    "        raise ValueError(\"⚠️ Please add MORALIS_API_KEY to your .env file!\")\n",
    "\n",
    "    wallet_address = \"0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045\"  # replace with user input\n",
    "    analyzer = ExtendedMoralisAnalyzer(API_KEY)\n",
    "\n",
    "    df = analyzer.get_detailed_data_for_wallet(wallet_address, max_per_chain=30)\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"No transactions found.\")\n",
    "    else:\n",
    "        print(df.head(20))\n",
    "        print(\"\\nSummary USD values:\")\n",
    "        print(df.groupby(\"transaction_type\")[\"usd_value\"].sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44da6b95",
   "metadata": {},
   "source": [
    "### analyzer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cfce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# -------------------------------\n",
    "# Cache for prices\n",
    "# -------------------------------\n",
    "class PriceCache:\n",
    "    def __init__(self, filename=\"price_cache.json\"):\n",
    "        self.filename = filename\n",
    "        self.cache = {}\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, \"r\") as f:\n",
    "                try:\n",
    "                    self.cache = json.load(f)\n",
    "                except:\n",
    "                    self.cache = {}\n",
    "\n",
    "    def get(self, key: str):\n",
    "        return self.cache.get(key)\n",
    "\n",
    "    def set(self, key: str, value):\n",
    "        self.cache[key] = value\n",
    "        with open(self.filename, \"w\") as f:\n",
    "            json.dump(self.cache, f)\n",
    "\n",
    "# -------------------------------\n",
    "# Cache for contract-to-CGID mapping\n",
    "# -------------------------------\n",
    "class AddressCache:\n",
    "    def __init__(self, filename=\"address_to_cgid.json\"):\n",
    "        self.filename = filename\n",
    "        self.cache = {}\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, \"r\") as f:\n",
    "                try:\n",
    "                    self.cache = json.load(f)\n",
    "                except:\n",
    "                    self.cache = {}\n",
    "\n",
    "    def get(self, key: str):\n",
    "        return self.cache.get(key)\n",
    "\n",
    "    def set(self, key: str, value):\n",
    "        self.cache[key] = value\n",
    "        with open(self.filename, \"w\") as f:\n",
    "            json.dump(self.cache, f)\n",
    "\n",
    "# -------------------------------\n",
    "# Extended Analyzer\n",
    "# -------------------------------\n",
    "class ExtendedMoralisAnalyzer:\n",
    "    def __init__(self, api_key: str, use_cache: bool = True, force_refresh: bool = False):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://deep-index.moralis.io/api/v2\"\n",
    "        self.headers = {\"Accept\": \"application/json\", \"X-API-Key\": api_key}\n",
    "\n",
    "        self.chains = {\n",
    "            'eth': '0x1',\n",
    "            'bsc': '0x38',\n",
    "            'polygon': '0x89',\n",
    "            'arbitrum': '0xa4b1',\n",
    "            'optimism': '0xa',\n",
    "            'base': '0x2105'\n",
    "        }\n",
    "\n",
    "        self.price_cache = PriceCache()\n",
    "        self.address_cache = AddressCache()\n",
    "\n",
    "    # -------------------------------\n",
    "    # Fetch ERC20 Transfers\n",
    "    # -------------------------------\n",
    "    def get_erc20_transfers(self, wallet: str, chain: str, limit: int = 50) -> List[Dict]:\n",
    "        try:\n",
    "            url = f\"{self.base_url}/{wallet}/erc20/transfers\"\n",
    "            params = {\"chain\": chain, \"limit\": limit}\n",
    "            response = requests.get(url, headers=self.headers, params=params)\n",
    "            if response.status_code == 200:\n",
    "                return response.json().get('result', [])\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"ERC20 transfer error: {e}\")\n",
    "            return []\n",
    "\n",
    "    # -------------------------------\n",
    "    # Fetch Tx Gas Cost (in native coin)\n",
    "    # -------------------------------\n",
    "    def get_tx_gas_cost(self, tx_hash: str, chain: str) -> Optional[float]:\n",
    "        try:\n",
    "            url = f\"{self.base_url}/transaction/{tx_hash}\"\n",
    "            params = {\"chain\": chain}\n",
    "            r = requests.get(url, headers=self.headers, params=params)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                gas_used = int(data.get(\"receipt_gas_used\") or 0)\n",
    "                gas_price = int(data.get(\"gas_price\") or 0)\n",
    "                native_spent = gas_used * gas_price / 1e18\n",
    "                return native_spent\n",
    "        except Exception as e:\n",
    "            print(f\"Gas fetch failed for {tx_hash}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # -------------------------------\n",
    "    # PRICE FETCHER (Coingecko)\n",
    "    # -------------------------------\n",
    "    def get_price_usd(self, symbol: str, timestamp: str, token_address: str = None, blockchain: str = \"ethereum\") -> Optional[float]:\n",
    "        if not token_address and not symbol:\n",
    "            return None\n",
    "\n",
    "        date_str = timestamp.split(\"T\")[0]\n",
    "        cache_key = f\"{token_address or symbol}_{date_str}\"\n",
    "        cached = self.price_cache.get(cache_key)\n",
    "        if cached:\n",
    "            return cached\n",
    "\n",
    "        cg_id = None\n",
    "\n",
    "        try:\n",
    "            # Step 1: prefer contract lookup first\n",
    "            if token_address:\n",
    "                # Check local address cache\n",
    "                cached_cgid = self.address_cache.get(token_address.lower())\n",
    "                if cached_cgid:\n",
    "                    cg_id = cached_cgid\n",
    "                else:\n",
    "                    # Query Coingecko contract API\n",
    "                    url = f\"https://api.coingecko.com/api/v3/coins/{blockchain}/contract/{token_address}\"\n",
    "                    r = requests.get(url)\n",
    "                    if r.status_code == 200:\n",
    "                        data = r.json()\n",
    "                        cg_id = data.get(\"id\")\n",
    "                        if cg_id:\n",
    "                            self.address_cache.set(token_address.lower(), cg_id)\n",
    "\n",
    "            # Step 2: fallback to hardcoded mapping if contract failed\n",
    "            if not cg_id and symbol:\n",
    "                mapping = {\n",
    "                    \"eth\": \"ethereum\",\n",
    "                    \"weth\": \"weth\",\n",
    "                    \"usdc\": \"usd-coin\",\n",
    "                    \"usdt\": \"tether\",\n",
    "                    \"bnb\": \"binancecoin\",\n",
    "                    \"matic\": \"polygon\"\n",
    "                }\n",
    "                cg_id = mapping.get(symbol.lower())\n",
    "\n",
    "            if not cg_id:\n",
    "                return None\n",
    "\n",
    "            # Step 3: fetch historical price\n",
    "            url = f\"https://api.coingecko.com/api/v3/coins/{cg_id}/history\"\n",
    "            params = {\"date\": datetime.strptime(date_str, \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")}\n",
    "            r = requests.get(url, params=params)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                price = data.get(\"market_data\", {}).get(\"current_price\", {}).get(\"usd\")\n",
    "                if price:\n",
    "                    self.price_cache.set(cache_key, price)\n",
    "                    return price\n",
    "        except Exception as e:\n",
    "            print(f\"Price fetch error for {symbol} / {token_address}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # -------------------------------\n",
    "    # Get current prices for unrealized PnL\n",
    "    # -------------------------------\n",
    "    def get_current_prices(self, tokens: List[Dict]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Fetch current USD prices for a list of tokens.\n",
    "        tokens: List of dicts with keys 'symbol', 'address', 'blockchain'\n",
    "        Returns: dict mapping token_address -> current_price_usd\n",
    "        \"\"\"\n",
    "        prices = {}\n",
    "        coingecko_ids = []\n",
    "        token_map = {}  # cg_id -> token_address\n",
    "        \n",
    "        for token in tokens:\n",
    "            symbol = token.get(\"symbol\", \"\")\n",
    "            address = token.get(\"address\", \"\")\n",
    "            blockchain = token.get(\"blockchain\", \"ethereum\")\n",
    "            \n",
    "            cache_key = f\"current_{address.lower()}\"\n",
    "            cached = self.price_cache.get(cache_key)\n",
    "            if cached:\n",
    "                prices[address] = cached\n",
    "                continue\n",
    "            \n",
    "            cg_id = None\n",
    "            \n",
    "            # Try to resolve Coingecko ID\n",
    "            if address:\n",
    "                cached_cgid = self.address_cache.get(address.lower())\n",
    "                if cached_cgid:\n",
    "                    cg_id = cached_cgid\n",
    "                else:\n",
    "                    try:\n",
    "                        url = f\"https://api.coingecko.com/api/v3/coins/{blockchain}/contract/{address}\"\n",
    "                        r = requests.get(url)\n",
    "                        if r.status_code == 200:\n",
    "                            data = r.json()\n",
    "                            cg_id = data.get(\"id\")\n",
    "                            if cg_id:\n",
    "                                self.address_cache.set(address.lower(), cg_id)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error resolving CG ID for {address}: {e}\")\n",
    "            \n",
    "            # Fallback to symbol mapping\n",
    "            if not cg_id and symbol:\n",
    "                mapping = {\n",
    "                    \"eth\": \"ethereum\",\n",
    "                    \"weth\": \"weth\", \n",
    "                    \"usdc\": \"usd-coin\",\n",
    "                    \"usdt\": \"tether\",\n",
    "                    \"bnb\": \"binancecoin\",\n",
    "                    \"matic\": \"polygon\",\n",
    "                    \"arb\": \"arbitrum\",\n",
    "                    \"op\": \"optimism\"\n",
    "                }\n",
    "                cg_id = mapping.get(symbol.lower())\n",
    "            \n",
    "            if cg_id:\n",
    "                coingecko_ids.append(cg_id)\n",
    "                token_map[cg_id] = address\n",
    "        \n",
    "        # Batch fetch current prices\n",
    "        if coingecko_ids:\n",
    "            try:\n",
    "                url = \"https://api.coingecko.com/api/v3/simple/price\"\n",
    "                params = {\n",
    "                    \"ids\": \",\".join(coingecko_ids),\n",
    "                    \"vs_currencies\": \"usd\"\n",
    "                }\n",
    "                r = requests.get(url, params=params)\n",
    "                if r.status_code == 200:\n",
    "                    data = r.json()\n",
    "                    for cg_id, price_data in data.items():\n",
    "                        if \"usd\" in price_data:\n",
    "                            token_address = token_map[cg_id]\n",
    "                            price = price_data[\"usd\"]\n",
    "                            prices[token_address] = price\n",
    "                            # Cache current prices briefly\n",
    "                            cache_key = f\"current_{token_address.lower()}\"\n",
    "                            self.price_cache.set(cache_key, price)\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching current prices: {e}\")\n",
    "        \n",
    "        return prices\n",
    "\n",
    "    # -------------------------------\n",
    "    # Enrich transfers with USD price + Gas cost\n",
    "    # -------------------------------\n",
    "    def get_detailed_data_for_wallet(self, wallet: str, max_per_chain: int = 50, chains: List[str] = None) -> pd.DataFrame:\n",
    "        all_tx = []\n",
    "        chains_to_fetch = chains if chains else list(self.chains.keys())\n",
    "        \n",
    "        # Known exchange addresses for better transaction classification\n",
    "        known_exchanges = {\n",
    "            '0x3f5ce5fbfe3e9af3971dd833d26ba9b5c936f0be',  # Binance\n",
    "            '0x742d35cc6634c0532925a3b844bc454e4438f44e',  # Bitfinex\n",
    "            '0x0681d8db095565fe8a346fa0277bffde9c0edbbf',  # BitMEX\n",
    "            '0x563b377a956c80d77a7c613a9343699ad6123911',  # Poloniex\n",
    "            # Add more as needed\n",
    "        }\n",
    "        \n",
    "        for chain_name in chains_to_fetch:\n",
    "            if chain_name not in self.chains:\n",
    "                continue\n",
    "                \n",
    "            chain_id = self.chains[chain_name]\n",
    "            print(f\"Fetching ERC20 transfers on {chain_name}...\")\n",
    "            erc20_txs = self.get_erc20_transfers(wallet, chain=chain_id, limit=max_per_chain)\n",
    "            \n",
    "            for tx in erc20_txs:\n",
    "                try:\n",
    "                    decimals = int(tx.get(\"token_decimals\") or 18)\n",
    "                    raw_value = float(tx.get(\"value\") or 0)\n",
    "                    amount = raw_value / (10 ** decimals)\n",
    "                    \n",
    "                    # Skip transactions with zero amount\n",
    "                    if amount <= 0:\n",
    "                        continue\n",
    "                        \n",
    "                    timestamp = tx.get(\"block_timestamp\")\n",
    "\n",
    "                    symbol = tx.get(\"token_symbol\", \"\")\n",
    "                    token_address = tx.get(\"address\", \"\")\n",
    "                    price_usd = self.get_price_usd(symbol, timestamp, token_address, chain_name) or 0\n",
    "                    usd_value = amount * price_usd\n",
    "\n",
    "                    # Gas cost (native coin)\n",
    "                    tx_hash = tx.get(\"transaction_hash\")\n",
    "                    gas_native = self.get_tx_gas_cost(tx_hash, chain_id) or 0\n",
    "                    \n",
    "                    # Enhanced transaction classification\n",
    "                    from_addr = tx.get(\"from_address\", \"\").lower()\n",
    "                    to_addr = tx.get(\"to_address\", \"\").lower()\n",
    "                    wallet_lower = wallet.lower()\n",
    "                    \n",
    "                    if to_addr == wallet_lower and from_addr == wallet_lower:\n",
    "                        tx_type = \"self_transfer\"\n",
    "                    elif to_addr == wallet_lower:\n",
    "                        tx_type = \"buy\" if from_addr in known_exchanges else \"incoming\"\n",
    "                    elif from_addr == wallet_lower:\n",
    "                        tx_type = \"sell\" if to_addr in known_exchanges else \"outgoing\"\n",
    "                    else:\n",
    "                        tx_type = \"external\"  # Should not happen for wallet-specific queries\n",
    "\n",
    "                    enriched = {\n",
    "                        \"wallet\": wallet,\n",
    "                        \"blockchain\": chain_name,\n",
    "                        \"tx_hash\": tx_hash,\n",
    "                        \"block_time\": timestamp,\n",
    "                        \"token_symbol\": symbol,\n",
    "                        \"token_address\": token_address,\n",
    "                        \"amount\": amount,\n",
    "                        \"price_usd\": price_usd,\n",
    "                        \"usd_value\": usd_value,\n",
    "                        \"gas_cost_native\": gas_native,\n",
    "                        \"transaction_type\": tx_type,\n",
    "                        \"from_address\": from_addr,\n",
    "                        \"to_address\": to_addr\n",
    "                    }\n",
    "                    all_tx.append(enriched)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing tx: {e}\")\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # rate limit\n",
    "\n",
    "        if not all_tx:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(all_tx)\n",
    "        df['block_time'] = pd.to_datetime(df['block_time'])\n",
    "        \n",
    "        # Clean up the index to be sequential without gaps\n",
    "        df = df.sort_values(\"block_time\").reset_index(drop=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "# -------------------------------\n",
    "# PnL CALCULATION\n",
    "# -------------------------------\n",
    "def calculate_pnl_improved(df: pd.DataFrame, method: str = \"FIFO\", analyzer: ExtendedMoralisAnalyzer = None) -> Tuple[float, float, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Improved PnL calculation that fetches current prices for unrealized PnL\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if df.empty:\n",
    "        return 0.0, 0.0, pd.DataFrame(columns=[\"Token\", \"Realized PnL (USD)\", \"Unrealized PnL (USD)\", \"Current Holdings\", \"Avg Cost\", \"Current Price\"])\n",
    "    \n",
    "    # Ensure proper sorting by time\n",
    "    df = df.sort_values(\"block_time\").reset_index(drop=True)\n",
    "    \n",
    "    positions = {}   # token -> list of lots (FIFO/LIFO)\n",
    "    avg_costs = {}   # token -> (total_qty, total_cost_basis) for ACB\n",
    "    realized_pnl = 0.0\n",
    "    token_realized = {}\n",
    "    \n",
    "    print(f\"Processing {len(df)} transactions using {method} method...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        token = row.get(\"token_symbol\", \"\")\n",
    "        if not token:\n",
    "            continue\n",
    "            \n",
    "        # Handle different quantity column names\n",
    "        qty = row.get(\"amount\", row.get(\"token_amount\", 0))\n",
    "        price = row.get(\"price_usd\", 0)\n",
    "        tx_type = row.get(\"transaction_type\", \"\")\n",
    "        \n",
    "        # Skip invalid transactions\n",
    "        if qty <= 0 or price <= 0 or pd.isna(price):\n",
    "            print(f\"Skipping invalid transaction: qty={qty}, price={price}, type={tx_type}\")\n",
    "            continue\n",
    "        \n",
    "        # Classify transactions into buys/sells\n",
    "        is_buy = tx_type in [\"deposit\", \"buy\", \"swap_in\", \"mint\", \"receive\"]\n",
    "        is_sell = tx_type in [\"withdrawal\", \"sell\", \"swap_out\", \"burn\", \"send\"]\n",
    "        \n",
    "        # Skip non-trading transactions\n",
    "        if not (is_buy or is_sell):\n",
    "            print(f\"Skipping non-trading transaction type: {tx_type}\")\n",
    "            continue\n",
    "            \n",
    "        # --- FIFO / LIFO Logic ---\n",
    "        if method in [\"FIFO\", \"LIFO\"]:\n",
    "            if is_buy:\n",
    "                # Add to position\n",
    "                lot = {\"qty\": float(qty), \"cost\": float(price)}\n",
    "                positions.setdefault(token, []).append(lot)\n",
    "                print(f\"Added lot: {qty} {token} @ ${price}\")\n",
    "                \n",
    "            elif is_sell:\n",
    "                # Sell from position\n",
    "                if token not in positions or not positions[token]:\n",
    "                    print(f\"WARNING: Selling {qty} {token} with no position!\")\n",
    "                    # Still record as realized loss (assuming cost basis = 0)\n",
    "                    pnl_piece = qty * price  # All proceeds are gain\n",
    "                    realized_pnl += pnl_piece\n",
    "                    token_realized[token] = token_realized.get(token, 0.0) + pnl_piece\n",
    "                    continue\n",
    "                \n",
    "                remaining_to_sell = float(qty)\n",
    "                sell_price = float(price)\n",
    "                \n",
    "                while remaining_to_sell > 0 and positions[token]:\n",
    "                    # Get lot based on method\n",
    "                    lot_idx = 0 if method == \"FIFO\" else -1\n",
    "                    lot = positions[token][lot_idx]\n",
    "                    \n",
    "                    lot_qty = lot[\"qty\"]\n",
    "                    lot_cost = lot[\"cost\"]\n",
    "                    \n",
    "                    # Determine how much to sell from this lot\n",
    "                    qty_to_sell = min(remaining_to_sell, lot_qty)\n",
    "                    \n",
    "                    # Calculate PnL for this portion\n",
    "                    proceeds = qty_to_sell * sell_price\n",
    "                    cost_basis = qty_to_sell * lot_cost\n",
    "                    pnl_piece = proceeds - cost_basis\n",
    "                    \n",
    "                    realized_pnl += pnl_piece\n",
    "                    token_realized[token] = token_realized.get(token, 0.0) + pnl_piece\n",
    "                    \n",
    "                    print(f\"Sold {qty_to_sell} {token}: ${proceeds:.2f} proceeds - ${cost_basis:.2f} cost = ${pnl_piece:.2f} PnL\")\n",
    "                    \n",
    "                    # Update lot and remaining\n",
    "                    lot[\"qty\"] -= qty_to_sell\n",
    "                    remaining_to_sell -= qty_to_sell\n",
    "                    \n",
    "                    # Remove empty lots\n",
    "                    if lot[\"qty\"] <= 0:\n",
    "                        positions[token].pop(lot_idx)\n",
    "        \n",
    "        # --- ACB (Average Cost Basis) Logic ---\n",
    "        elif method == \"ACB\":\n",
    "            if is_buy:\n",
    "                # Update average cost basis\n",
    "                current_qty, current_total_cost = avg_costs.get(token, (0.0, 0.0))\n",
    "                new_qty = current_qty + qty\n",
    "                new_total_cost = current_total_cost + (qty * price)\n",
    "                avg_costs[token] = (new_qty, new_total_cost)\n",
    "                print(f\"ACB updated for {token}: {new_qty} units, avg cost = ${new_total_cost/new_qty:.4f}\")\n",
    "                \n",
    "            elif is_sell:\n",
    "                current_qty, current_total_cost = avg_costs.get(token, (0.0, 0.0))\n",
    "                \n",
    "                if current_qty <= 0:\n",
    "                    print(f\"WARNING: Selling {qty} {token} with no ACB position!\")\n",
    "                    # Treat as all gain\n",
    "                    pnl_piece = qty * price\n",
    "                    realized_pnl += pnl_piece\n",
    "                    token_realized[token] = token_realized.get(token, 0.0) + pnl_piece\n",
    "                    continue\n",
    "                \n",
    "                # Calculate average cost\n",
    "                avg_cost = current_total_cost / current_qty if current_qty > 0 else 0\n",
    "                \n",
    "                # Calculate PnL\n",
    "                qty_to_sell = min(qty, current_qty)  # Can't sell more than we have\n",
    "                proceeds = qty_to_sell * price\n",
    "                cost_basis = qty_to_sell * avg_cost\n",
    "                pnl_piece = proceeds - cost_basis\n",
    "                \n",
    "                realized_pnl += pnl_piece\n",
    "                token_realized[token] = token_realized.get(token, 0.0) + pnl_piece\n",
    "                \n",
    "                print(f\"ACB sale: {qty_to_sell} {token} @ ${price} vs avg cost ${avg_cost:.4f} = ${pnl_piece:.2f} PnL\")\n",
    "                \n",
    "                # Update position\n",
    "                new_qty = max(0, current_qty - qty_to_sell)\n",
    "                new_total_cost = max(0, current_total_cost - (qty_to_sell * avg_cost))\n",
    "                avg_costs[token] = (new_qty, new_total_cost)\n",
    "    \n",
    "    # --- Calculate Unrealized PnL with CURRENT PRICES ---\n",
    "    print(\"\\nCalculating unrealized PnL with current market prices...\")\n",
    "    unrealized_pnl = 0.0\n",
    "    token_unrealized = {}\n",
    "    token_holdings = {}\n",
    "    current_prices = {}\n",
    "    \n",
    "    # Collect tokens for current price lookup\n",
    "    tokens_for_current_prices = []\n",
    "    \n",
    "    if method in [\"FIFO\", \"LIFO\"]:\n",
    "        for token, lots in positions.items():\n",
    "            if lots:\n",
    "                # Get token info from the dataframe\n",
    "                token_df = df[df[\"token_symbol\"] == token]\n",
    "                if not token_df.empty:\n",
    "                    token_row = token_df.iloc[0]\n",
    "                    tokens_for_current_prices.append({\n",
    "                        \"symbol\": token,\n",
    "                        \"address\": token_row.get(\"token_address\", \"\"),\n",
    "                        \"blockchain\": token_row.get(\"blockchain\", \"ethereum\")\n",
    "                    })\n",
    "    \n",
    "    elif method == \"ACB\":\n",
    "        for token, (total_qty, _) in avg_costs.items():\n",
    "            if total_qty > 0:\n",
    "                token_df = df[df[\"token_symbol\"] == token]\n",
    "                if not token_df.empty:\n",
    "                    token_row = token_df.iloc[0]\n",
    "                    tokens_for_current_prices.append({\n",
    "                        \"symbol\": token,\n",
    "                        \"address\": token_row.get(\"token_address\", \"\"),\n",
    "                        \"blockchain\": token_row.get(\"blockchain\", \"ethereum\")\n",
    "                    })\n",
    "    \n",
    "    # Fetch current prices if analyzer is available\n",
    "    if analyzer and tokens_for_current_prices:\n",
    "        try:\n",
    "            current_prices = analyzer.get_current_prices(tokens_for_current_prices)\n",
    "            print(f\"Fetched current prices for {len(current_prices)} tokens\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching current prices: {e}\")\n",
    "            # Fallback to last transaction prices\n",
    "            for token in set(df[\"token_symbol\"]):\n",
    "                token_df = df[df[\"token_symbol\"] == token]\n",
    "                if not token_df.empty:\n",
    "                    token_addr = token_df.iloc[-1][\"token_address\"]\n",
    "                    current_prices[token_addr] = token_df.iloc[-1][\"price_usd\"]\n",
    "    else:\n",
    "        # Fallback: use last transaction prices\n",
    "        print(\"Using last transaction prices as current prices\")\n",
    "        for token in set(df[\"token_symbol\"]):\n",
    "            token_df = df[df[\"token_symbol\"] == token]\n",
    "            if not token_df.empty:\n",
    "                token_addr = token_df.iloc[-1][\"token_address\"]\n",
    "                current_prices[token_addr] = token_df.iloc[-1][\"price_usd\"]\n",
    "    \n",
    "    # Calculate unrealized PnL for FIFO/LIFO\n",
    "    if method in [\"FIFO\", \"LIFO\"]:\n",
    "        for token, lots in positions.items():\n",
    "            if not lots:\n",
    "                continue\n",
    "                \n",
    "            # Get token info for price lookup\n",
    "            token_df = df[df[\"token_symbol\"] == token]\n",
    "            if token_df.empty:\n",
    "                continue\n",
    "                \n",
    "            token_addr = token_df.iloc[0][\"token_address\"]\n",
    "            current_price = current_prices.get(token_addr, 0)\n",
    "            \n",
    "            if current_price <= 0:\n",
    "                print(f\"No current price available for {token}, skipping unrealized PnL\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate total quantity and cost basis\n",
    "            total_qty = sum(lot[\"qty\"] for lot in lots)\n",
    "            total_cost_basis = sum(lot[\"qty\"] * lot[\"cost\"] for lot in lots)\n",
    "            \n",
    "            # Calculate unrealized PnL\n",
    "            current_value = total_qty * current_price\n",
    "            unrealized_pnl_token = current_value - total_cost_basis\n",
    "            \n",
    "            unrealized_pnl += unrealized_pnl_token\n",
    "            token_unrealized[token] = unrealized_pnl_token\n",
    "            token_holdings[token] = {\n",
    "                \"qty\": total_qty,\n",
    "                \"avg_cost\": total_cost_basis / total_qty if total_qty > 0 else 0,\n",
    "                \"current_price\": current_price,\n",
    "                \"current_value\": current_value\n",
    "            }\n",
    "            \n",
    "            print(f\"{token}: {total_qty:.4f} units @ avg ${total_cost_basis/total_qty:.4f}, current ${current_price:.4f} = ${unrealized_pnl_token:.2f} unrealized\")\n",
    "    \n",
    "    # Calculate unrealized PnL for ACB\n",
    "    elif method == \"ACB\":\n",
    "        for token, (total_qty, total_cost_basis) in avg_costs.items():\n",
    "            if total_qty <= 0:\n",
    "                continue\n",
    "                \n",
    "            token_df = df[df[\"token_symbol\"] == token]\n",
    "            if token_df.empty:\n",
    "                continue\n",
    "            \n",
    "            token_addr = token_df.iloc[0][\"token_address\"]\n",
    "            current_price = current_prices.get(token_addr, 0)\n",
    "            \n",
    "            if current_price <= 0:\n",
    "                print(f\"No current price available for {token}, skipping unrealized PnL\")\n",
    "                continue\n",
    "            \n",
    "            avg_cost = total_cost_basis / total_qty if total_qty > 0 else 0\n",
    "            \n",
    "            current_value = total_qty * current_price\n",
    "            unrealized_pnl_token = current_value - total_cost_basis\n",
    "            \n",
    "            unrealized_pnl += unrealized_pnl_token\n",
    "            token_unrealized[token] = unrealized_pnl_token\n",
    "            token_holdings[token] = {\n",
    "                \"qty\": total_qty,\n",
    "                \"avg_cost\": avg_cost,\n",
    "                \"current_price\": current_price,\n",
    "                \"current_value\": current_value\n",
    "            }\n",
    "            \n",
    "            print(f\"{token}: {total_qty:.4f} units @ avg ${avg_cost:.4f}, current ${current_price:.4f} = ${unrealized_pnl_token:.2f} unrealized\")\n",
    "    \n",
    "    # --- Build detailed breakdown ---\n",
    "    all_tokens = set(token_realized.keys()).union(token_unrealized.keys())\n",
    "    token_data = []\n",
    "    \n",
    "    for token in all_tokens:\n",
    "        holdings = token_holdings.get(token, {})\n",
    "        token_data.append({\n",
    "            \"Token\": token,\n",
    "            \"Realized PnL (USD)\": round(token_realized.get(token, 0.0), 2),\n",
    "            \"Unrealized PnL (USD)\": round(token_unrealized.get(token, 0.0), 2),\n",
    "            \"Current Holdings\": round(holdings.get(\"qty\", 0.0), 6),\n",
    "            \"Avg Cost\": round(holdings.get(\"avg_cost\", 0.0), 4),\n",
    "            \"Current Price\": round(holdings.get(\"current_price\", 0.0), 4),\n",
    "            \"Current Value\": round(holdings.get(\"current_value\", 0.0), 2)\n",
    "        })\n",
    "    \n",
    "    breakdown_df = pd.DataFrame(token_data)\n",
    "    if not breakdown_df.empty:\n",
    "        breakdown_df = breakdown_df.sort_values(\"Realized PnL (USD)\", ascending=False)\n",
    "    \n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Realized PnL: ${realized_pnl:.2f}\")\n",
    "    print(f\"Unrealized PnL: ${unrealized_pnl:.2f}\")\n",
    "    print(f\"Total PnL: ${realized_pnl + unrealized_pnl:.2f}\")\n",
    "    \n",
    "    return realized_pnl, unrealized_pnl, breakdown_df\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# PnL VALIDATION FUNCTION\n",
    "# -------------------------------\n",
    "def validate_pnl_calculation(df: pd.DataFrame, realized_pnl: float, unrealized_pnl: float, breakdown_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Validate PnL calculations for reasonableness and consistency.\n",
    "    \"\"\"\n",
    "    validation_results = []\n",
    "    \n",
    "    # Check 1: Total PnL components should sum correctly\n",
    "    breakdown_realized_sum = breakdown_df[\"Realized PnL (USD)\"].sum() if not breakdown_df.empty else 0\n",
    "    breakdown_unrealized_sum = breakdown_df[\"Unrealized PnL (USD)\"].sum() if not breakdown_df.empty else 0\n",
    "    \n",
    "    validation_results.append({\n",
    "        \"Check\": \"PnL Components Sum\",\n",
    "        \"Expected Realized\": realized_pnl,\n",
    "        \"Breakdown Realized\": breakdown_realized_sum,\n",
    "        \"Expected Unrealized\": unrealized_pnl,\n",
    "        \"Breakdown Unrealized\": breakdown_unrealized_sum,\n",
    "        \"Pass\": abs(realized_pnl - breakdown_realized_sum) < 0.01 and abs(unrealized_pnl - breakdown_unrealized_sum) < 0.01\n",
    "    })\n",
    "    \n",
    "    # Check 2: No negative holdings\n",
    "    if not breakdown_df.empty:\n",
    "        negative_holdings = breakdown_df[breakdown_df[\"Current Holdings\"] < 0]\n",
    "        validation_results.append({\n",
    "            \"Check\": \"No Negative Holdings\",\n",
    "            \"Negative Count\": len(negative_holdings),\n",
    "            \"Pass\": len(negative_holdings) == 0\n",
    "        })\n",
    "    \n",
    "    # Check 3: Reasonable price ranges\n",
    "    if not df.empty:\n",
    "        price_stats = df[\"price_usd\"].describe()\n",
    "        validation_results.append({\n",
    "            \"Check\": \"Price Range Reasonableness\",\n",
    "            \"Min Price\": price_stats[\"min\"],\n",
    "            \"Max Price\": price_stats[\"max\"],\n",
    "            \"Pass\": price_stats[\"min\"] >= 0 and price_stats[\"max\"] < 1000000  # Basic sanity check\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(validation_results)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Simple token price fetcher (fallback)\n",
    "# -------------------------------\n",
    "def get_token_price(token_symbol: str) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Simple fallback function to get token prices.\n",
    "    In a real implementation, this would use an API like CoinGecko.\n",
    "    \"\"\"\n",
    "    # This is a simple mock implementation\n",
    "    price_mapping = {\n",
    "        \"ETH\": 3000.0,\n",
    "        \"BTC\": 60000.0,\n",
    "        \"USDC\": 1.0,\n",
    "        \"USDT\": 1.0,\n",
    "        \"DAI\": 1.0,\n",
    "        \"WBTC\": 60000.0,\n",
    "        \"WETH\": 3000.0,\n",
    "    }\n",
    "    return price_mapping.get(token_symbol.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4373627e",
   "metadata": {},
   "source": [
    "### app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc33d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "from analyzer import ExtendedMoralisAnalyzer, calculate_pnl_improved, validate_pnl_calculation\n",
    "from price_fetcher import get_token_price\n",
    "\n",
    "import numpy as np\n",
    "import pytz\n",
    "import random\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "\n",
    "# Sample Data\n",
    "def generate_sample_data(n_days=7, txs_per_day=7, wallet_address=\"0xDEADBEEF1234567890ABCDEF1234567890ABCDEF\"):\n",
    "    np.random.seed(42)  # reproducible\n",
    "    rows = []\n",
    "    start_date = datetime.today() - timedelta(days=n_days)\n",
    "\n",
    "    tokens = [\n",
    "        (\"USDC\", 1.0),\n",
    "        (\"USDT\", 1.0), \n",
    "        (\"ETH\", 2000.0),\n",
    "        (\"ARB\", 3.0),\n",
    "        (\"OP\", 3.5),\n",
    "        (\"MATIC\", 0.7),\n",
    "    ]\n",
    "    chains = [\"eth\", \"arbitrum\", \"optimism\", \"polygon\"]\n",
    "\n",
    "    tx_types = [\"deposit\", \"withdrawal\", \"buy\", \"sell\"]\n",
    "\n",
    "    for d in range(n_days):\n",
    "        for _ in range(txs_per_day):\n",
    "            block_time = start_date + timedelta(days=d, hours=np.random.randint(0, 24))\n",
    "            token, base_price = tokens[np.random.randint(len(tokens))]\n",
    "            \n",
    "            # Generate slightly different current vs historical prices for unrealized PnL demo\n",
    "            historical_price = round(base_price * np.random.uniform(0.85, 1.15), 2)\n",
    "            \n",
    "            amount = round(np.random.uniform(10, 1000), 2) if token in [\"USDC\", \"USDT\"] else round(np.random.uniform(0.1, 20), 4)\n",
    "\n",
    "            tx_type = np.random.choice(tx_types)\n",
    "\n",
    "            # USD value logic\n",
    "            if tx_type in [\"deposit\", \"buy\"]:\n",
    "                usd_value = amount * historical_price\n",
    "            elif tx_type in [\"withdrawal\", \"sell\"]:\n",
    "                usd_value = amount * historical_price * np.random.uniform(0.95, 1.05)\n",
    "            else:\n",
    "                usd_value = amount * historical_price\n",
    "\n",
    "            # fake addresses\n",
    "            from_addr = f\"0x{random.randint(10**15, 10**18):x}\"\n",
    "            to_addr = f\"0x{random.randint(10**15, 10**18):x}\"\n",
    "\n",
    "            # Simulate some withdrawals going back to your own wallet\n",
    "            if tx_type == \"withdrawal\" and random.random() < 0.3:  # 30% chance\n",
    "                to_addr = wallet_address\n",
    "                tx_type = \"withdrawal_move\"\n",
    "\n",
    "            rows.append({\n",
    "                \"tx_hash\": f\"0x{random.randint(10**15, 10**18):x}\",\n",
    "                \"block_time\": block_time,\n",
    "                \"blockchain\": np.random.choice(chains),\n",
    "                \"transaction_type\": tx_type,\n",
    "                \"amount\": amount,               \n",
    "                \"price_usd\": historical_price,             \n",
    "                \"usd_value\": usd_value,         \n",
    "                \"gas_cost_usd\": round(np.random.uniform(1, 20), 2),\n",
    "                \"token_symbol\": token,\n",
    "                \"token_address\": f\"0x{random.randint(10**15, 10**18):x}\",\n",
    "                \"from_address\": from_addr,\n",
    "                \"to_address\": to_addr,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Mock current prices for sample data (slightly different from historical for demo)\n",
    "def get_sample_current_prices():\n",
    "    \"\"\"Generate mock current prices that differ from historical prices\"\"\"\n",
    "    return {\n",
    "        # These would be token addresses in real data, using symbols for demo\n",
    "        \"USDC\": 1.00,\n",
    "        \"USDT\": 0.999,\n",
    "        \"ETH\": 2150.0,  # Higher than historical average\n",
    "        \"ARB\": 2.85,    # Lower than historical average  \n",
    "        \"OP\": 3.75,     # Higher than historical average\n",
    "        \"MATIC\": 0.72,  # Slightly higher\n",
    "    }\n",
    "\n",
    "# Create ~50 transactions across a week\n",
    "sample_df = generate_sample_data(n_days=7, txs_per_day=7)\n",
    "\n",
    "# -------------------------------\n",
    "# Setup\n",
    "# -------------------------------\n",
    "st.set_page_config(page_title=\"Wallet PnL Explorer\", page_icon=\"💰\", layout=\"wide\")\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"MORALIS_API_KEY\")\n",
    "if not API_KEY:\n",
    "    st.error(\"⚠️ Please add MORALIS_API_KEY to your .env file!\")\n",
    "    st.stop()\n",
    "\n",
    "CACHE_DIR = \"cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Disk cache helpers (per-wallet + per-chain)\n",
    "# -------------------------------\n",
    "def _wallet_dir(wallet: str):\n",
    "    return os.path.join(CACHE_DIR, wallet.lower())\n",
    "\n",
    "def save_to_disk(wallet: str, chain: str, df: pd.DataFrame):\n",
    "    wdir = _wallet_dir(wallet)\n",
    "    os.makedirs(wdir, exist_ok=True)\n",
    "    path = os.path.join(wdir, f\"{chain}.parquet\")\n",
    "    df.to_parquet(path, index=False)\n",
    "\n",
    "def load_from_disk(wallet: str, chain: str):\n",
    "    path = os.path.join(_wallet_dir(wallet), f\"{chain}.parquet\")\n",
    "    if os.path.exists(path):\n",
    "        return pd.read_parquet(path)\n",
    "    return None\n",
    "\n",
    "@st.cache_data(show_spinner=False)\n",
    "def get_wallet_data(_analyzer, wallet: str, chains: list, max_txs: int, force_refresh: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Hybrid memory+disk+API cache. Returns concatenated df for requested chains.\"\"\"\n",
    "    dfs = []\n",
    "    for ch in chains:\n",
    "        if not force_refresh:\n",
    "            cached = load_from_disk(wallet, ch)\n",
    "            if cached is not None:\n",
    "                dfs.append(cached)\n",
    "                continue\n",
    "\n",
    "        # API call for that chain\n",
    "        try:\n",
    "            df = _analyzer.get_detailed_data_for_wallet(wallet, max_per_chain=max_txs, chains=[ch])\n",
    "        except TypeError:\n",
    "            # Fallback: fetch all and filter by chain\n",
    "            df_all = _analyzer.get_detailed_data_for_wallet(wallet, max_per_chain=max_txs)\n",
    "            df = df_all[df_all[\"blockchain\"] == ch] if not df_all.empty else df_all\n",
    "\n",
    "        if not df.empty:\n",
    "            save_to_disk(wallet, ch, df)\n",
    "            dfs.append(df)\n",
    "\n",
    "    if dfs:\n",
    "        return pd.concat(dfs, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def main():\n",
    "    st.title(\"Wallet PnL Explorer\")\n",
    "    st.sidebar.header(\"🔧 Controls\")\n",
    "\n",
    "    # Sidebar controls\n",
    "    diagnostic_mode = st.sidebar.checkbox(\"Enable Diagnostic Mode\", value=False)\n",
    "    pnl_method = st.sidebar.selectbox(\"PnL Accounting Method\", [\"FIFO\", \"LIFO\", \"ACB\"], index=0)\n",
    "    wallet_address = st.sidebar.text_input(\"Wallet Address\", value=\"\", help=\"Leave empty to preview demo data.\")\n",
    "    selected_chains = st.sidebar.multiselect(\n",
    "        \"Blockchains\",\n",
    "        [\"eth\", \"bsc\", \"polygon\", \"arbitrum\", \"optimism\", \"base\"],\n",
    "        default=[\"eth\", \"arbitrum\", \"optimism\"]\n",
    "    )\n",
    "    start_date = st.sidebar.date_input(\"Start Date\", value=(datetime.utcnow() - timedelta(days=30)).date())\n",
    "    end_date = st.sidebar.date_input(\"End Date\", value=datetime.utcnow().date())\n",
    "    max_txs = st.sidebar.slider(\"Max transactions per chain\", min_value=10, max_value=200, value=50, step=10)\n",
    "    cache_mode = st.sidebar.radio(\"Cache Mode\", [\"Always Use Cache\", \"Force Refresh\", \"Disable Cache\"], index=0)\n",
    "    analyze_button = st.sidebar.button(\"🔍 Analyze Wallet\")\n",
    "\n",
    "    # Initialize analyzer\n",
    "    if cache_mode == \"Always Use Cache\":\n",
    "        analyzer = ExtendedMoralisAnalyzer(API_KEY, use_cache=True, force_refresh=False)\n",
    "        force_refresh = False\n",
    "    elif cache_mode == \"Force Refresh\":\n",
    "        analyzer = ExtendedMoralisAnalyzer(API_KEY, use_cache=True, force_refresh=True)\n",
    "        force_refresh = True\n",
    "    else:\n",
    "        analyzer = ExtendedMoralisAnalyzer(API_KEY, use_cache=False)\n",
    "        force_refresh = False\n",
    "\n",
    "    # Determine wallet mode\n",
    "    if analyze_button and wallet_address.strip():\n",
    "        chosen_wallet = wallet_address.strip()\n",
    "        using_default = False\n",
    "        window_start = datetime.combine(start_date, datetime.min.time())\n",
    "        window_end = datetime.combine(end_date, datetime.max.time())\n",
    "    else:\n",
    "        chosen_wallet = \"sample_wallet\"\n",
    "        using_default = True\n",
    "        window_start = sample_df[\"block_time\"].min()\n",
    "        window_end = sample_df[\"block_time\"].max()\n",
    "        st.info(\"💡 Sample wallet preview for the past 7 days: Enter your wallet on the left to analyze real data.\")\n",
    "\n",
    "    if not selected_chains:\n",
    "        st.warning(\"Please select at least one blockchain in the sidebar.\")\n",
    "        st.stop()\n",
    "\n",
    "    # Fetch/load wallet data\n",
    "    if using_default:\n",
    "        df = sample_df.copy()\n",
    "    else:\n",
    "        progress = st.progress(0, text=\"Preparing analysis...\")\n",
    "        progress.progress(20, text=\"Checking cache / fetching data...\")\n",
    "        df = get_wallet_data(analyzer, chosen_wallet, selected_chains, max_txs, force_refresh=force_refresh)\n",
    "        progress.progress(50, text=\"Applying filters...\")\n",
    "\n",
    "        if df.empty:\n",
    "            progress.empty()\n",
    "            st.error(\"No transactions found for this wallet.\")\n",
    "            st.stop()\n",
    "\n",
    "        # Ensure UTC datetime\n",
    "        if df[\"block_time\"].dt.tz is None:\n",
    "            df[\"block_time\"] = df[\"block_time\"].dt.tz_localize(\"UTC\")\n",
    "        window_start = pd.Timestamp(window_start).tz_localize(\"UTC\")\n",
    "        window_end = pd.Timestamp(window_end).tz_localize(\"UTC\")\n",
    "\n",
    "        # Filter by date window\n",
    "        df = df[(df[\"block_time\"] >= window_start) & (df[\"block_time\"] <= window_end)]\n",
    "\n",
    "        # Keep only tokens with valid prices\n",
    "        df = df[df[\"price_usd\"].notna() & (df[\"price_usd\"] > 0)]\n",
    "\n",
    "        # Mark withdrawals to your own wallet as moves\n",
    "        df['transaction_type'] = df.apply(\n",
    "            lambda row: 'withdrawal_move'\n",
    "            if row['transaction_type'] == 'withdrawal' and str(row.get('to_address', '')).lower() == chosen_wallet.lower()\n",
    "            else row['transaction_type'],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        progress.progress(70, text=\"Computing summaries and PnL...\")\n",
    "\n",
    "    if df.empty:\n",
    "        st.warning(\"⚠️ No transactions available after filters.\")\n",
    "        st.stop()\n",
    "\n",
    "    # -------------------------------\n",
    "    # Summary metrics\n",
    "    # -------------------------------\n",
    "    total_in = float(df[df[\"transaction_type\"] == \"deposit\"][\"usd_value\"].sum())\n",
    "    total_out = float(df[df[\"transaction_type\"] == \"withdrawal\"][\"usd_value\"].sum())\n",
    "    gas_cost = float(df.get(\"gas_cost_usd\", pd.Series()).fillna(0).sum()) if \"gas_cost_usd\" in df else 0.0\n",
    "    pnl = total_in - total_out - gas_cost\n",
    "\n",
    "    col1, col2, col3, col4 = st.columns(4)\n",
    "    col1.metric(\"Total Deposits (USD)\", f\"${total_in:,.2f}\")\n",
    "    col2.metric(\"Total Withdrawals (USD)\", f\"${total_out:,.2f}\")\n",
    "    col3.metric(\"Gas Costs (USD)\", f\"${gas_cost:,.2f}\")\n",
    "    col4.metric(\"Net Cash Flow (USD)\", f\"${pnl:,.2f}\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # PnL calculation with current prices\n",
    "    # -------------------------------\n",
    "    realized_total = 0.0\n",
    "    unrealized_total = 0.0\n",
    "    breakdown_list = []\n",
    "\n",
    "    # Group by token_symbol for sample data, token_address for real data\n",
    "    group_key = 'token_symbol' if using_default else 'token_address'\n",
    "    grouped = df.groupby(group_key)\n",
    "\n",
    "    tokens_with_valid_prices = set()\n",
    "    tokens_with_missing_prices = set()\n",
    "\n",
    "    for token_key, group in grouped:\n",
    "        # Filter group to rows with valid prices\n",
    "        group_valid = group[group['price_usd'].notna() & (group['price_usd'] > 0)]\n",
    "        if group_valid.empty:\n",
    "            tokens_with_missing_prices.add(token_key)\n",
    "            continue\n",
    "\n",
    "        tokens_with_valid_prices.add(token_key)\n",
    "\n",
    "        # Calculate PnL for this token group - PASS ANALYZER for current prices\n",
    "        if using_default:\n",
    "            # For sample data, create a mock analyzer that returns sample current prices\n",
    "            class MockAnalyzer:\n",
    "                def get_current_prices(self, tokens):\n",
    "                    sample_prices = get_sample_current_prices()\n",
    "                    result = {}\n",
    "                    for token in tokens:\n",
    "                        symbol = token.get('symbol', '')\n",
    "                        result[token.get('address', symbol)] = sample_prices.get(symbol, 0)\n",
    "                    return result\n",
    "            \n",
    "            mock_analyzer = MockAnalyzer()\n",
    "            realized, unrealized, breakdown = calculate_pnl_improved(group_valid, method=pnl_method, analyzer=mock_analyzer)\n",
    "        else:\n",
    "            realized, unrealized, breakdown = calculate_pnl_improved(group_valid, method=pnl_method, analyzer=analyzer)\n",
    "\n",
    "        realized_total += realized\n",
    "        unrealized_total += unrealized\n",
    "        breakdown_list.append(breakdown)\n",
    "\n",
    "    # Combine breakdowns into one DataFrame\n",
    "    if breakdown_list:\n",
    "        breakdown_df = pd.concat(breakdown_list, ignore_index=True)\n",
    "    else:\n",
    "        breakdown_df = pd.DataFrame()\n",
    "\n",
    "    if tokens_with_missing_prices:\n",
    "        st.warning(f\"Tokens excluded from PnL due to missing prices: {tokens_with_missing_prices}\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # PnL Validation\n",
    "    # -------------------------------\n",
    "    validation_df = validate_pnl_calculation(df[df[group_key].isin(tokens_with_valid_prices)], realized_total, unrealized_total, breakdown_df)\n",
    "    failed_validations = validation_df[validation_df['Pass'] == False]\n",
    "\n",
    "    if not failed_validations.empty:\n",
    "        st.warning(\"⚠️ PnL Validation Issues Detected\")\n",
    "        with st.expander(\"View Validation Details\", expanded=True):\n",
    "            st.dataframe(validation_df, use_container_width=True)\n",
    "            st.write(\"**Issues found:**\")\n",
    "            for _, row in failed_validations.iterrows():\n",
    "                st.write(f\"- {row['Check']}: Failed\")\n",
    "    else:\n",
    "        st.success(\"✅ PnL Calculations Validated Successfully\")\n",
    "        with st.expander(\"View Validation Details\"):\n",
    "            st.dataframe(validation_df, use_container_width=True)\n",
    "  \n",
    "    col1, col2, col3 = st.columns(3)\n",
    "    col1.metric(f\"{pnl_method} Realized PnL (USD)\", f\"${realized_total:,.2f}\")\n",
    "    col2.metric(f\"{pnl_method} Unrealized PnL (USD)\", f\"${unrealized_total:,.2f}\")\n",
    "    \n",
    "    # Count open positions\n",
    "    open_positions = len(breakdown_df[breakdown_df['Current Holdings'] > 0]) if not breakdown_df.empty else 0\n",
    "    col3.metric(\"Open Positions\", f\"{open_positions}\")\n",
    "\n",
    "    st.subheader(\"💹 PnL Breakdown by Token\")\n",
    "    if not breakdown_df.empty:\n",
    "        st.dataframe(breakdown_df, use_container_width=True, height=320)\n",
    "    else:\n",
    "        st.info(\"No PnL data available.\")\n",
    "     \n",
    "    # -------------------------------\n",
    "    # Transactions table\n",
    "    # -------------------------------\n",
    "    st.subheader(\"📊 Enriched Transactions\")\n",
    "    st.dataframe(df, use_container_width=True, height=420)\n",
    "\n",
    "    from price_fetcher import get_token_price\n",
    "\n",
    "    # Replace invalid prices with historical fetch\n",
    "    df[\"price_usd\"] = df.apply(\n",
    "        lambda row: row[\"price_usd\"]\n",
    "        if row[\"price_usd\"] > 0\n",
    "        else (\n",
    "            get_token_price(\n",
    "                row[\"token_symbol\"],\n",
    "                row.get(\"token_address\"),\n",
    "                row[\"blockchain\"],\n",
    "                block_time=row[\"block_time\"]\n",
    "            ) or 0\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Drop unsupported tokens (no price found)\n",
    "    df = df[df[\"price_usd\"] > 0]\n",
    "\n",
    "\n",
    "\n",
    "    if not df.empty:\n",
    "        realized_pnl, unrealized_pnl, breakdown_df = calculate_pnl_improved(\n",
    "        df, method=pnl_method, analyzer=analyzer\n",
    "    )\n",
    "\n",
    "    if diagnostic_mode:\n",
    "        st.subheader(\"🔍 Diagnostic Report\")\n",
    "        validation_df = validate_pnl_calculation(df, realized_pnl, unrealized_pnl, breakdown_df)\n",
    "        st.dataframe(validation_df, use_container_width=True)\n",
    "\n",
    "        st.subheader(\"📊 First 10 Transactions\")\n",
    "        st.dataframe(df.head(10), use_container_width=True)\n",
    "\n",
    "        st.subheader(\"📊 PnL Breakdown\")\n",
    "        st.dataframe(breakdown_df, use_container_width=True)\n",
    "\n",
    "\n",
    "    if not using_default:\n",
    "        with st.expander(\"📂 View local cache files\"):\n",
    "            files = glob.glob(os.path.join(CACHE_DIR, chosen_wallet.lower(), \"*.parquet\"))\n",
    "            st.write([os.path.basename(f) for f in files]) if files else st.write(\"No cache files yet.\")\n",
    "\n",
    "        progress.progress(100, text=\"Done!\")\n",
    "        time.sleep(0.1)\n",
    "        progress.empty()\n",
    "      \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7fd5ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# -------------------------------\n",
    "# Cache for prices\n",
    "# -------------------------------\n",
    "class PriceCache:\n",
    "    def __init__(self, filename=\"price_cache.json\"):\n",
    "        self.filename = filename\n",
    "        self.cache = {}\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, \"r\") as f:\n",
    "                try:\n",
    "                    self.cache = json.load(f)\n",
    "                except:\n",
    "                    self.cache = {}\n",
    "\n",
    "    def get(self, key: str):\n",
    "        return self.cache.get(key)\n",
    "\n",
    "    def set(self, key: str, value):\n",
    "        self.cache[key] = value\n",
    "        with open(self.filename, \"w\") as f:\n",
    "            json.dump(self.cache, f)\n",
    "\n",
    "# -------------------------------\n",
    "# Cache for contract-to-CGID mapping\n",
    "# -------------------------------\n",
    "class AddressCache:\n",
    "    def __init__(self, filename=\"address_to_cgid.json\"):\n",
    "        self.filename = filename\n",
    "        self.cache = {}\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, \"r\") as f:\n",
    "                try:\n",
    "                    self.cache = json.load(f)\n",
    "                except:\n",
    "                    self.cache = {}\n",
    "\n",
    "    def get(self, key: str):\n",
    "        return self.cache.get(key)\n",
    "\n",
    "    def set(self, key: str, value):\n",
    "        self.cache[key] = value\n",
    "        with open(self.filename, \"w\") as f:\n",
    "            json.dump(self.cache, f)\n",
    "\n",
    "# -------------------------------\n",
    "# Extended Analyzer\n",
    "# -------------------------------\n",
    "class ExtendedMoralisAnalyzer:\n",
    "    def __init__(self, api_key: str, use_cache: bool = True, force_refresh: bool = False):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://deep-index.moralis.io/api/v2\"\n",
    "        self.headers = {\"Accept\": \"application/json\", \"X-API-Key\": api_key}\n",
    "\n",
    "        self.chains = {\n",
    "            'eth': '0x1',\n",
    "            'bsc': '0x38',\n",
    "            'polygon': '0x89',\n",
    "            'arbitrum': '0xa4b1',\n",
    "            'optimism': '0xa',\n",
    "            'base': '0x2105'\n",
    "        }\n",
    "\n",
    "        self.price_cache = PriceCache()\n",
    "        self.address_cache = AddressCache()\n",
    "\n",
    "    # -------------------------------\n",
    "    # Fetch ERC20 Transfers\n",
    "    # -------------------------------\n",
    "    def get_erc20_transfers(self, wallet: str, chain: str, limit: int = 50) -> List[Dict]:\n",
    "        try:\n",
    "            url = f\"{self.base_url}/{wallet}/erc20/transfers\"\n",
    "            params = {\"chain\": chain, \"limit\": limit}\n",
    "            response = requests.get(url, headers=self.headers, params=params)\n",
    "            if response.status_code == 200:\n",
    "                return response.json().get('result', [])\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"ERC20 transfer error: {e}\")\n",
    "            return []\n",
    "\n",
    "    # -------------------------------\n",
    "    # Fetch Tx Gas Cost (in native coin)\n",
    "    # -------------------------------\n",
    "    def get_tx_gas_cost(self, tx_hash: str, chain: str) -> Optional[float]:\n",
    "        try:\n",
    "            url = f\"{self.base_url}/transaction/{tx_hash}\"\n",
    "            params = {\"chain\": chain}\n",
    "            r = requests.get(url, headers=self.headers, params=params)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                gas_used = int(data.get(\"receipt_gas_used\") or 0)\n",
    "                gas_price = int(data.get(\"gas_price\") or 0)\n",
    "                native_spent = gas_used * gas_price / 1e18\n",
    "                return native_spent\n",
    "        except Exception as e:\n",
    "            print(f\"Gas fetch failed for {tx_hash}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # -------------------------------\n",
    "    # PRICE FETCHER (Coingecko)\n",
    "    # -------------------------------\n",
    "    def get_price_usd(self, symbol: str, timestamp: str, token_address: str = None, blockchain: str = \"ethereum\") -> Optional[float]:\n",
    "        if not token_address and not symbol:\n",
    "            return None\n",
    "\n",
    "        date_str = timestamp.split(\"T\")[0]\n",
    "        cache_key = f\"{token_address or symbol}_{date_str}\"\n",
    "        cached = self.price_cache.get(cache_key)\n",
    "        if cached:\n",
    "            return cached\n",
    "\n",
    "        cg_id = None\n",
    "\n",
    "        try:\n",
    "            # Step 1: prefer contract lookup first\n",
    "            if token_address:\n",
    "                # Check local address cache\n",
    "                cached_cgid = self.address_cache.get(token_address.lower())\n",
    "                if cached_cgid:\n",
    "                    cg_id = cached_cgid\n",
    "                else:\n",
    "                    # Query Coingecko contract API\n",
    "                    url = f\"https://api.coingecko.com/api/v3/coins/{blockchain}/contract/{token_address}\"\n",
    "                    r = requests.get(url)\n",
    "                    if r.status_code == 200:\n",
    "                        data = r.json()\n",
    "                        cg_id = data.get(\"id\")\n",
    "                        if cg_id:\n",
    "                            self.address_cache.set(token_address.lower(), cg_id)\n",
    "\n",
    "            # Step 2: fallback to hardcoded mapping if contract failed\n",
    "            if not cg_id and symbol:\n",
    "                mapping = {\n",
    "                    \"eth\": \"ethereum\",\n",
    "                    \"weth\": \"weth\",\n",
    "                    \"usdc\": \"usd-coin\",\n",
    "                    \"usdt\": \"tether\",\n",
    "                    \"bnb\": \"binancecoin\",\n",
    "                    \"matic\": \"polygon\"\n",
    "                }\n",
    "                cg_id = mapping.get(symbol.lower())\n",
    "\n",
    "            if not cg_id:\n",
    "                return None\n",
    "\n",
    "            # Step 3: fetch historical price\n",
    "            url = f\"https://api.coingecko.com/api/v3/coins/{cg_id}/history\"\n",
    "            params = {\"date\": datetime.strptime(date_str, \"%Y-%m-%d\").strftime(\"%d-%m-%Y\")}\n",
    "            r = requests.get(url, params=params)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                price = data.get(\"market_data\", {}).get(\"current_price\", {}).get(\"usd\")\n",
    "                if price:\n",
    "                    self.price_cache.set(cache_key, price)\n",
    "                    return price\n",
    "        except Exception as e:\n",
    "            print(f\"Price fetch error for {symbol} / {token_address}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # -------------------------------\n",
    "    # NEW: Get current prices for unrealized PnL\n",
    "    # -------------------------------\n",
    "    def get_current_prices(self, tokens: List[Dict]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Fetch current USD prices for a list of tokens.\n",
    "        tokens: List of dicts with keys 'symbol', 'address', 'blockchain'\n",
    "        Returns: dict mapping token_address -> current_price_usd\n",
    "        \"\"\"\n",
    "        prices = {}\n",
    "        coingecko_ids = []\n",
    "        token_map = {}  # cg_id -> token_address\n",
    "        \n",
    "        for token in tokens:\n",
    "            symbol = token.get(\"symbol\", \"\")\n",
    "            address = token.get(\"address\", \"\")\n",
    "            blockchain = token.get(\"blockchain\", \"ethereum\")\n",
    "            \n",
    "            cache_key = f\"current_{address.lower()}\"\n",
    "            cached = self.price_cache.get(cache_key)\n",
    "            if cached:\n",
    "                prices[address] = cached\n",
    "                continue\n",
    "            \n",
    "            cg_id = None\n",
    "            \n",
    "            # Try to resolve Coingecko ID\n",
    "            if address:\n",
    "                cached_cgid = self.address_cache.get(address.lower())\n",
    "                if cached_cgid:\n",
    "                    cg_id = cached_cgid\n",
    "                else:\n",
    "                    try:\n",
    "                        url = f\"https://api.coingecko.com/api/v3/coins/{blockchain}/contract/{address}\"\n",
    "                        r = requests.get(url)\n",
    "                        if r.status_code == 200:\n",
    "                            data = r.json()\n",
    "                            cg_id = data.get(\"id\")\n",
    "                            if cg_id:\n",
    "                                self.address_cache.set(address.lower(), cg_id)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error resolving CG ID for {address}: {e}\")\n",
    "            \n",
    "            # Fallback to symbol mapping\n",
    "            if not cg_id and symbol:\n",
    "                mapping = {\n",
    "                    \"eth\": \"ethereum\",\n",
    "                    \"weth\": \"weth\", \n",
    "                    \"usdc\": \"usd-coin\",\n",
    "                    \"usdt\": \"tether\",\n",
    "                    \"bnb\": \"binancecoin\",\n",
    "                    \"matic\": \"polygon\",\n",
    "                    \"arb\": \"arbitrum\",\n",
    "                    \"op\": \"optimism\"\n",
    "                }\n",
    "                cg_id = mapping.get(symbol.lower())\n",
    "            \n",
    "            if cg_id:\n",
    "                coingecko_ids.append(cg_id)\n",
    "                token_map[cg_id] = address\n",
    "        \n",
    "        # Batch fetch current prices\n",
    "        if coingecko_ids:\n",
    "            try:\n",
    "                url = \"https://api.coingecko.com/api/v3/simple/price\"\n",
    "                params = {\n",
    "                    \"ids\": \",\".join(coingecko_ids),\n",
    "                    \"vs_currencies\": \"usd\"\n",
    "                }\n",
    "                r = requests.get(url, params=params)\n",
    "                if r.status_code == 200:\n",
    "                    data = r.json()\n",
    "                    for cg_id, price_data in data.items():\n",
    "                        if \"usd\" in price_data:\n",
    "                            token_address = token_map[cg_id]\n",
    "                            price = price_data[\"usd\"]\n",
    "                            prices[token_address] = price\n",
    "                            # Cache current prices briefly\n",
    "                            cache_key = f\"current_{token_address.lower()}\"\n",
    "                            self.price_cache.set(cache_key, price)\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching current prices: {e}\")\n",
    "        \n",
    "        return prices\n",
    "\n",
    "    # -------------------------------\n",
    "    # Enrich transfers with USD price + Gas cost\n",
    "    # -------------------------------\n",
    "    def get_detailed_data_for_wallet(self, wallet: str, max_per_chain: int = 50, chains: List[str] = None) -> pd.DataFrame:\n",
    "        all_tx = []\n",
    "        chains_to_fetch = chains if chains else list(self.chains.keys())\n",
    "        \n",
    "        for chain_name in chains_to_fetch:\n",
    "            if chain_name not in self.chains:\n",
    "                continue\n",
    "                \n",
    "            chain_id = self.chains[chain_name]\n",
    "            print(f\"Fetching ERC20 transfers on {chain_name}...\")\n",
    "            erc20_txs = self.get_erc20_transfers(wallet, chain=chain_id, limit=max_per_chain)\n",
    "            \n",
    "            for tx in erc20_txs:\n",
    "                try:\n",
    "                    decimals = int(tx.get(\"token_decimals\") or 18)\n",
    "                    raw_value = float(tx.get(\"value\") or 0)\n",
    "                    amount = raw_value / (10 ** decimals)\n",
    "                    timestamp = tx.get(\"block_timestamp\")\n",
    "\n",
    "                    symbol = tx.get(\"token_symbol\", \"\")\n",
    "                    token_address = tx.get(\"address\", \"\")\n",
    "                    price_usd = self.get_price_usd(symbol, timestamp, token_address, chain_name) or 0\n",
    "                    usd_value = amount * price_usd\n",
    "\n",
    "                    # Gas cost (native coin)\n",
    "                    tx_hash = tx.get(\"transaction_hash\")\n",
    "                    gas_native = self.get_tx_gas_cost(tx_hash, chain_id) or 0\n",
    "\n",
    "                    enriched = {\n",
    "                        \"wallet\": wallet,\n",
    "                        \"blockchain\": chain_name,\n",
    "                        \"tx_hash\": tx_hash,\n",
    "                        \"block_time\": timestamp,\n",
    "                        \"token_symbol\": symbol,\n",
    "                        \"token_address\": token_address,\n",
    "                        \"amount\": amount,\n",
    "                        \"price_usd\": price_usd,\n",
    "                        \"usd_value\": usd_value,\n",
    "                        \"gas_cost_native\": gas_native,\n",
    "                        \"transaction_type\": \"deposit\" if tx.get(\"to_address\", \"\").lower() == wallet.lower() else \"withdrawal\"\n",
    "                    }\n",
    "                    all_tx.append(enriched)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing tx: {e}\")\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # rate limit\n",
    "\n",
    "        if not all_tx:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(all_tx)\n",
    "        df['block_time'] = pd.to_datetime(df['block_time'])\n",
    "        return df.sort_values(\"block_time\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# FIXED PnL CALCULATION\n",
    "# -------------------------------\n",
    "def calculate_pnl_improved(df, method=\"FIFO\", analyzer=None):\n",
    "    \"\"\"\n",
    "    FIXED: Improved PnL calculation that fetches current prices for unrealized PnL\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if df.empty:\n",
    "        return 0, 0, pd.DataFrame(columns=[\"Token\", \"Realized PnL (USD)\", \"Unrealized PnL (USD)\", \"Current Holdings\", \"Avg Cost\", \"Current Price\"])\n",
    "    \n",
    "    # Ensure proper sorting by time\n",
    "    df = df.sort_values(\"block_time\").reset_index(drop=True)\n",
    "    \n",
    "    positions = {}   # token -> list of lots (FIFO/LIFO)\n",
    "    avg_costs = {}   # token -> (total_qty, total_cost_basis) for ACB\n",
    "    realized_pnl = 0\n",
    "    token_realized = {}\n",
    "    token_unrealized = {}\n",
    "    \n",
    "    print(f\"Processing {len(df)} transactions using {method} method...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        token = row.get(\"token_symbol\", \"\")\n",
    "        if not token:\n",
    "            continue\n",
    "            \n",
    "        # Handle different quantity column names\n",
    "        qty = row.get(\"amount\", row.get(\"token_amount\", 0))\n",
    "        price = row.get(\"price_usd\", 0)\n",
    "        tx_type = row.get(\"transaction_type\", \"\")\n",
    "        \n",
    "        # Skip invalid transactions\n",
    "        if qty <= 0 or price <= 0 or pd.isna(price):\n",
    "            print(f\"Skipping invalid transaction: qty={qty}, price={price}, type={tx_type}\")\n",
    "            continue\n",
    "        \n",
    "        # Classify transactions into buys/sells\n",
    "        is_buy = tx_type in [\"deposit\", \"buy\", \"swap_in\", \"mint\", \"receive\"]\n",
    "        is_sell = tx_type in [\"withdrawal\", \"sell\", \"swap_out\", \"burn\", \"send\"]\n",
    "        \n",
    "        # Skip non-trading transactions\n",
    "        if not (is_buy or is_sell):\n",
    "            print(f\"Skipping non-trading transaction type: {tx_type}\")\n",
    "            continue\n",
    "            \n",
    "        # --- FIFO / LIFO Logic ---\n",
    "        if method in [\"FIFO\", \"LIFO\"]:\n",
    "            if is_buy:\n",
    "                # Add to position\n",
    "                lot = {\"qty\": float(qty), \"cost\": float(price)}\n",
    "                positions.setdefault(token, []).append(lot)\n",
    "                print(f\"Added lot: {qty} {token} @ ${price}\")\n",
    "                \n",
    "            elif is_sell:\n",
    "                # Sell from position\n",
    "                if token not in positions or not positions[token]:\n",
    "                    print(f\"WARNING: Selling {qty} {token} with no position!\")\n",
    "                    # Still record as realized loss (assuming cost basis = 0)\n",
    "                    pnl_piece = qty * price  # All proceeds are gain\n",
    "                    realized_pnl += pnl_piece\n",
    "                    token_realized[token] = token_realized.get(token, 0) + pnl_piece\n",
    "                    continue\n",
    "                \n",
    "                remaining_to_sell = float(qty)\n",
    "                sell_price = float(price)\n",
    "                \n",
    "                while remaining_to_sell > 0 and positions[token]:\n",
    "                    # Get lot based on method\n",
    "                    lot_idx = 0 if method == \"FIFO\" else -1\n",
    "                    lot = positions[token][lot_idx]\n",
    "                    \n",
    "                    lot_qty = lot[\"qty\"]\n",
    "                    lot_cost = lot[\"cost\"]\n",
    "                    \n",
    "                    # Determine how much to sell from this lot\n",
    "                    qty_to_sell = min(remaining_to_sell, lot_qty)\n",
    "                    \n",
    "                    # Calculate PnL for this portion\n",
    "                    proceeds = qty_to_sell * sell_price\n",
    "                    cost_basis = qty_to_sell * lot_cost\n",
    "                    pnl_piece = proceeds - cost_basis\n",
    "                    \n",
    "                    realized_pnl += pnl_piece\n",
    "                    token_realized[token] = token_realized.get(token, 0) + pnl_piece\n",
    "                    \n",
    "                    print(f\"Sold {qty_to_sell} {token}: ${proceeds:.2f} proceeds - ${cost_basis:.2f} cost = ${pnl_piece:.2f} PnL\")\n",
    "                    \n",
    "                    # Update lot and remaining\n",
    "                    lot[\"qty\"] -= qty_to_sell\n",
    "                    remaining_to_sell -= qty_to_sell\n",
    "                    \n",
    "                    # Remove empty lots\n",
    "                    if lot[\"qty\"] <= 0:\n",
    "                        positions[token].pop(lot_idx)\n",
    "        \n",
    "        # --- ACB (Average Cost Basis) Logic ---\n",
    "        elif method == \"ACB\":\n",
    "            if is_buy:\n",
    "                # Update average cost basis\n",
    "                current_qty, current_total_cost = avg_costs.get(token, (0, 0))\n",
    "                new_qty = current_qty + qty\n",
    "                new_total_cost = current_total_cost + (qty * price)\n",
    "                avg_costs[token] = (new_qty, new_total_cost)\n",
    "                print(f\"ACB updated for {token}: {new_qty} units, avg cost = ${new_total_cost/new_qty:.4f}\")\n",
    "                \n",
    "            elif is_sell:\n",
    "                current_qty, current_total_cost = avg_costs.get(token, (0, 0))\n",
    "                \n",
    "                if current_qty <= 0:\n",
    "                    print(f\"WARNING: Selling {qty} {token} with no ACB position!\")\n",
    "                    # Treat as all gain\n",
    "                    pnl_piece = qty * price\n",
    "                    realized_pnl += pnl_piece\n",
    "                    token_realized[token] = token_realized.get(token, 0) + pnl_piece\n",
    "                    continue\n",
    "                \n",
    "                # Calculate average cost\n",
    "                avg_cost = current_total_cost / current_qty if current_qty > 0 else 0\n",
    "                \n",
    "                # Calculate PnL\n",
    "                qty_to_sell = min(qty, current_qty)  # Can't sell more than we have\n",
    "                proceeds = qty_to_sell * price\n",
    "                cost_basis = qty_to_sell * avg_cost\n",
    "                pnl_piece = proceeds - cost_basis\n",
    "                \n",
    "                realized_pnl += pnl_piece\n",
    "                token_realized[token] = token_realized.get(token, 0) + pnl_piece\n",
    "                \n",
    "                print(f\"ACB sale: {qty_to_sell} {token} @ ${price} vs avg cost ${avg_cost:.4f} = ${pnl_piece:.2f} PnL\")\n",
    "                \n",
    "                # Update position\n",
    "                new_qty = max(0, current_qty - qty_to_sell)\n",
    "                new_total_cost = max(0, current_total_cost - (qty_to_sell * avg_cost))\n",
    "                avg_costs[token] = (new_qty, new_total_cost)\n",
    "    \n",
    "    # --- Calculate Unrealized PnL with CURRENT PRICES ---\n",
    "    print(\"\\nCalculating unrealized PnL with current market prices...\")\n",
    "    unrealized_pnl = 0\n",
    "    token_holdings = {}\n",
    "    current_prices = {}\n",
    "    \n",
    "    # Collect tokens for current price lookup\n",
    "    tokens_for_current_prices = []\n",
    "    if method in [\"FIFO\", \"LIFO\"]:\n",
    "        for token, lots in positions.items():\n",
    "            if lots:\n",
    "                # Get token info from the dataframe\n",
    "                token_row = df[df[\"token_symbol\"] == token].iloc[0]\n",
    "                tokens_for_current_prices.append({\n",
    "                    \"symbol\": token,\n",
    "                    \"address\": token_row.get(\"token_address\", \"\"),\n",
    "                    \"blockchain\": token_row.get(\"blockchain\", \"ethereum\")\n",
    "                })\n",
    "    \n",
    "    elif method == \"ACB\":\n",
    "        for token, (total_qty, _) in avg_costs.items():\n",
    "            if total_qty > 0:\n",
    "                token_row = df[df[\"token_symbol\"] == token].iloc[0]\n",
    "                tokens_for_current_prices.append({\n",
    "                    \"symbol\": token,\n",
    "                    \"address\": token_row.get(\"token_address\", \"\"),\n",
    "                    \"blockchain\": token_row.get(\"blockchain\", \"ethereum\")\n",
    "                })\n",
    "    \n",
    "    # Fetch current prices if analyzer is available\n",
    "    if analyzer and tokens_for_current_prices:\n",
    "        try:\n",
    "            current_prices = analyzer.get_current_prices(tokens_for_current_prices)\n",
    "            print(f\"Fetched current prices for {len(current_prices)} tokens\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching current prices: {e}\")\n",
    "            # Fallback to last transaction prices\n",
    "            for token in set(df[\"token_symbol\"]):\n",
    "                token_df = df[df[\"token_symbol\"] == token]\n",
    "                if not token_df.empty:\n",
    "                    token_addr = token_df.iloc[-1][\"token_address\"]\n",
    "                    current_prices[token_addr] = token_df.iloc[-1][\"price_usd\"]\n",
    "    else:\n",
    "        # Fallback: use last transaction prices\n",
    "        print(\"Using last transaction prices as current prices\")\n",
    "        for token in set(df[\"token_symbol\"]):\n",
    "            token_df = df[df[\"token_symbol\"] == token]\n",
    "            if not token_df.empty:\n",
    "                token_addr = token_df.iloc[-1][\"token_address\"]\n",
    "                current_prices[token_addr] = token_df.iloc[-1][\"price_usd\"]\n",
    "    \n",
    "    # Calculate unrealized PnL with current prices\n",
    "    if method in [\"FIFO\", \"LIFO\"]:\n",
    "        for token, lots in positions.items():\n",
    "            if not lots:\n",
    "                continue\n",
    "                \n",
    "            token_df = df[df[\"token_symbol\"] == token]\n",
    "            if token_df.empty:\n",
    "                continue\n",
    "            \n",
    "            token_addr = token_df.iloc[0][\"token_address\"]\n",
    "            current_price = current_prices.get(token_addr, 0)\n",
    "            \n",
    "            if current_price <= 0:\n",
    "                print(f\"No current price available for {token}, skipping unrealized PnL\")\n",
    "                continue\n",
    "            \n",
    "            total_qty = sum(lot[\"qty\"] for lot in lots)\n",
    "            total_cost_basis = sum(lot[\"qty\"] * lot[\"cost\"] for lot in lots)\n",
    "            avg_cost = total_cost_basis / total_qty if total_qty > 0 else 0\n",
    "            \n",
    "            current_value = total_qty * current_price\n",
    "            unrealized_pnl_token = current_value - total_cost_basis\n",
    "            \n",
    "            unrealized_pnl += unrealized_pnl_token\n",
    "            token_unrealized[token] = unrealized_pnl_token\n",
    "            token_holdings[token] = {\n",
    "                \"qty\": total_qty,\n",
    "                \"avg_cost\": avg_cost,\n",
    "                \"current_price\": current_price,\n",
    "                \"current_value\": current_value\n",
    "            }\n",
    "            \n",
    "            print(f\"{token}: {total_qty:.4f} units @ avg ${avg_cost:.4f}, current ${current_price:.4f} = ${unrealized_pnl_token:.2f} unrealized\")\n",
    "    \n",
    "    elif method == \"ACB\":\n",
    "        for token, (total_qty, total_cost_basis) in avg_costs.items():\n",
    "            if total_qty <= 0:\n",
    "                continue\n",
    "                \n",
    "            token_df = df[df[\"token_symbol\"] == token]\n",
    "            if token_df.empty:\n",
    "                continue\n",
    "            \n",
    "            token_addr = token_df.iloc[0][\"token_address\"]\n",
    "            current_price = current_prices.get(token_addr, 0)\n",
    "            \n",
    "            if current_price <= 0:\n",
    "                print(f\"No current price available for {token}, skipping unrealized PnL\")\n",
    "                continue\n",
    "            \n",
    "            avg_cost = total_cost_basis / total_qty if total_qty > 0 else 0\n",
    "            \n",
    "            current_value = total_qty * current_price\n",
    "            unrealized_pnl_token = current_value - total_cost_basis\n",
    "            \n",
    "            unrealized_pnl += unrealized_pnl_token\n",
    "            token_unrealized[token] = unrealized_pnl_token\n",
    "            token_holdings[token] = {\n",
    "                \"qty\": total_qty,\n",
    "                \"avg_cost\": avg_cost,\n",
    "                \"current_price\": current_price,\n",
    "                \"current_value\": current_value\n",
    "            }\n",
    "            \n",
    "            print(f\"{token}: {total_qty:.4f} units @ avg ${avg_cost:.4f}, current ${current_price:.4f} = ${unrealized_pnl_token:.2f} unrealized\")\n",
    "    \n",
    "    # --- Build detailed breakdown ---\n",
    "    all_tokens = set(token_realized.keys()).union(token_unrealized.keys())\n",
    "    token_data = []\n",
    "    \n",
    "    for token in all_tokens:\n",
    "        holdings = token_holdings.get(token, {})\n",
    "        token_data.append({\n",
    "            \"Token\": token,\n",
    "            \"Realized PnL (USD)\": round(token_realized.get(token, 0), 2),\n",
    "            \"Unrealized PnL (USD)\": round(token_unrealized.get(token, 0), 2),\n",
    "            \"Current Holdings\": round(holdings.get(\"qty\", 0), 6),\n",
    "            \"Avg Cost\": round(holdings.get(\"avg_cost\", 0), 4),\n",
    "            \"Current Price\": round(holdings.get(\"current_price\", 0), 4),\n",
    "            \"Current Value\": round(holdings.get(\"current_value\", 0), 2)\n",
    "        })\n",
    "    \n",
    "    breakdown_df = pd.DataFrame(token_data)\n",
    "    if not breakdown_df.empty:\n",
    "        breakdown_df = breakdown_df.sort_values(\"Realized PnL (USD)\", ascending=False)\n",
    "    \n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Realized PnL: ${realized_pnl:.2f}\")\n",
    "    print(f\"Unrealized PnL: ${unrealized_pnl:.2f}\")\n",
    "    print(f\"Total PnL: ${realized_pnl + unrealized_pnl:.2f}\")\n",
    "    \n",
    "    return realized_pnl, unrealized_pnl, breakdown_df\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# PnL VALIDATION FUNCTION\n",
    "# -------------------------------\n",
    "def validate_pnl_calculation(df, realized_pnl, unrealized_pnl, breakdown_df):\n",
    "    \"\"\"\n",
    "    Validate PnL calculations for reasonableness and consistency.\n",
    "    \"\"\"\n",
    "    validation_results = []\n",
    "    \n",
    "    # Check 1: Total PnL components should sum correctly\n",
    "    breakdown_realized_sum = breakdown_df[\"Realized PnL (USD)\"].sum() if not breakdown_df.empty else 0\n",
    "    breakdown_unrealized_sum = breakdown_df[\"Unrealized PnL (USD)\"].sum() if not breakdown_df.empty else 0\n",
    "    \n",
    "    validation_results.append({\n",
    "        \"Check\": \"PnL Components Sum\",\n",
    "        \"Expected Realized\": realized_pnl,\n",
    "        \"Breakdown Realized\": breakdown_realized_sum,\n",
    "        \"Expected Unrealized\": unrealized_pnl,\n",
    "        \"Breakdown Unrealized\": breakdown_unrealized_sum,\n",
    "        \"Pass\": abs(realized_pnl - breakdown_realized_sum) < 0.01 and abs(unrealized_pnl - breakdown_unrealized_sum) < 0.01\n",
    "    })\n",
    "    \n",
    "    # Check 2: No negative holdings\n",
    "    if not breakdown_df.empty:\n",
    "        negative_holdings = breakdown_df[breakdown_df[\"Current Holdings\"] < 0]\n",
    "        validation_results.append({\n",
    "            \"Check\": \"No Negative Holdings\",\n",
    "            \"Negative Count\": len(negative_holdings),\n",
    "            \"Pass\": len(negative_holdings) == 0\n",
    "        })\n",
    "    \n",
    "    # Check 3: Reasonable price ranges\n",
    "    if not df.empty:\n",
    "        price_stats = df[\"price_usd\"].describe()\n",
    "        validation_results.append({\n",
    "            \"Check\": \"Price Range Reasonableness\",\n",
    "            \"Min Price\": price_stats[\"min\"],\n",
    "            \"Max Price\": price_stats[\"max\"],\n",
    "            \"Pass\": price_stats[\"min\"] >= 0 and price_stats[\"max\"] < 1000000  # Basic sanity check\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d882aecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching ERC20 transfers on optimism...\n",
      "Fetching ERC20 transfers on arbitrum...\n",
      "Processing 12 transactions using FIFO method...\n",
      "Skipping invalid transaction: qty=84.562542, price=0.0, type=deposit\n",
      "Skipping invalid transaction: qty=1.30867, price=0.0, type=deposit\n",
      "Skipping invalid transaction: qty=3000.0, price=0.0, type=deposit\n",
      "Skipping invalid transaction: qty=84.0, price=0.0, type=withdrawal\n",
      "Added lot: 29.765 USDT @ $1.000413034018476\n",
      "Added lot: 86.91201014728924 WCT @ $0.34449833013901\n",
      "Sold 29.765 USDT: $29.78 proceeds - $29.78 cost = $0.00 PnL\n",
      "Skipping invalid transaction: qty=0.0001, price=0.0, type=deposit\n",
      "Sold 86.91201014728924 WCT: $29.94 proceeds - $29.94 cost = $0.00 PnL\n",
      "Added lot: 44.88 WCT @ $0.3292533159924128\n",
      "Added lot: 0.003635087554407808 WCT @ $0.2955672492818812\n",
      "Added lot: 86.91201014728924 WCT @ $0.289267648020578\n",
      "\n",
      "Calculating unrealized PnL with current market prices...\n",
      "Fetched current prices for 0 tokens\n",
      "No current price available for WCT, skipping unrealized PnL\n",
      "\n",
      "Final Results:\n",
      "Realized PnL: $0.00\n",
      "Unrealized PnL: $0.00\n",
      "Total PnL: $0.00\n",
      "Validation Results\n",
      "                        Check  Expected Realized  Breakdown Realized  \\\n",
      "0          PnL Components Sum                0.0                 0.0   \n",
      "1        No Negative Holdings                NaN                 NaN   \n",
      "2  Price Range Reasonableness                NaN                 NaN   \n",
      "\n",
      "   Expected Unrealized  Breakdown Unrealized  Pass  Negative Count  Min Price  \\\n",
      "0                  0.0                   0.0  True             NaN        NaN   \n",
      "1                  NaN                   NaN  True             0.0        NaN   \n",
      "2                  NaN                   NaN  True             NaN        0.0   \n",
      "\n",
      "   Max Price  \n",
      "0        NaN  \n",
      "1        NaN  \n",
      "2   1.000413  \n",
      "\n",
      "Breakdown\n",
      "  Token  Realized PnL (USD)  Unrealized PnL (USD)  Current Holdings  Avg Cost  \\\n",
      "0   WCT                 0.0                     0                 0         0   \n",
      "1  USDT                 0.0                     0                 0         0   \n",
      "\n",
      "   Current Price  Current Value  \n",
      "0              0              0  \n",
      "1              0              0  \n"
     ]
    }
   ],
   "source": [
    "analyzer = ExtendedMoralisAnalyzer(api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJub25jZSI6ImIxMzVkNTc0LTE3MDItNGFlOS05ZGFhLWM5ZGI3NDZmOWQ2ZCIsIm9yZ0lkIjoiNDY5MDQ1IiwidXNlcklkIjoiNDgyNTI5IiwidHlwZUlkIjoiMmNlOWY1NTItOGQ1Ni00YzgyLTk1ZTctOWY4ZDkyYWE4MGJhIiwidHlwZSI6IlBST0pFQ1QiLCJpYXQiOjE3NTY5NTExMjgsImV4cCI6NDkxMjcxMTEyOH0.-zQFPB_ZeVUQ2daD6jgnLMsUhcyRpX7ghUnTRaxAlNw\")\n",
    "df = analyzer.get_detailed_data_for_wallet(\"0x0b23B218c08dD2156CEb19aF5bB765096D73BA70\", max_per_chain=30, chains=[\"bnb\",\"optimism\",\"arbitrum\"])\n",
    "\n",
    "realized, unrealized, breakdown = calculate_pnl_improved(df, method=\"FIFO\", analyzer=analyzer)\n",
    "\n",
    "validation = validate_pnl_calculation(df, realized, unrealized, breakdown)\n",
    "\n",
    "print(\"Validation Results\")\n",
    "print(validation)\n",
    "print(\"\\nBreakdown\")\n",
    "print(breakdown.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WalletPnL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
